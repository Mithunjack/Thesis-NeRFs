{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda Launch blocking\n",
    "**`%env CUDA_LAUNCH_BLOCKING=1`** \n",
    "sets a variable that makes CUDA GPU operations run sequentially/synchronously in Jupyter notebooks for simplicity, despite reducing parallel performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seeds for the entire session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(17)\n",
    "np.random.seed(17)\n",
    "torch.manual_seed(17)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ActiveVisionLab/nerfmm.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import general NeRF-related functions, including:\n",
    "\n",
    "- positional encoding\n",
    "- volume sampling\n",
    "- volume rendering\n",
    "- ray direction computing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfmm.utils.pos_enc import encode_position\n",
    "from nerfmm.utils.volume_op import volume_rendering, volume_sampling_ndc\n",
    "from nerfmm.utils.comp_ray_dir import comp_ray_dir_cam_fxfy\n",
    "\n",
    "# utlities\n",
    "from nerfmm.utils.training_utils import mse2psnr\n",
    "from nerfmm.utils.lie_group_helper import convert3x4_4x4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset_name = 'Dataset_7'\n",
    "dataset_folder = 'Datasets/RGB'  \n",
    "scene_name = 'Blender_Data' \n",
    "image_dir = os.path.join(dataset_folder, custom_dataset_name)\n",
    "\n",
    "# Count the number of images in the dataset folder\n",
    "num_images = len([f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "\n",
    "print('Selected custom dataset: \"{}\"'.format(custom_dataset_name))\n",
    "print('Dataset path: {}'.format(image_dir))\n",
    "print('Number of images in the dataset: {}'.format(num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing `Imageio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio \n",
    "def load_imgs(image_dir):\n",
    "    img_names = np.array(sorted(os.listdir(image_dir)))  # all image names\n",
    "    img_paths = [os.path.join(image_dir, n) for n in img_names]\n",
    "    N_imgs = len(img_paths)\n",
    "\n",
    "    img_list = []\n",
    "    for p in img_paths:\n",
    "        img = imageio.imread(p)[:, :, :3]  # (H, W, 3) np.uint8\n",
    "        img_list.append(img)\n",
    "    img_list = np.stack(img_list)  # (N, H, W, 3)\n",
    "    img_list = torch.from_numpy(img_list).float() / 255  # (N, H, W, 3) torch.float32\n",
    "    H, W = img_list.shape[1], img_list.shape[2]\n",
    "    \n",
    "    results = {\n",
    "        'imgs': img_list,  # (N, H, W, 3) torch.float32\n",
    "        'img_names': img_names,  # (N, )\n",
    "        'N_imgs': N_imgs,\n",
    "        'H': H,\n",
    "        'W': W,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "image_data = load_imgs(image_dir)\n",
    "imgs = image_data['imgs']  # (N, H, W, 3) torch.float32\n",
    "\n",
    "N_IMGS = image_data['N_imgs']\n",
    "H = image_data['H']\n",
    "W = image_data['W']\n",
    "\n",
    "print('Loaded {0} imgs, resolution {1} x {2}'.format(N_IMGS, H, W))\n",
    "print(imgs.shape)\n",
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define learnable FOCALS\n",
    "\n",
    "We initialise $f_x = W$ and $f_y = H$. \n",
    "\n",
    "In practice, two coeffcients are initialised at $1.0$ and multiplied with the input image size. We also found a $2^{nd}$-order trick provides slightly better results, but it's not necessary. For more details about the $2^{nd}$-order trick, see the supplementary section in our [arxiv paper](https://https://arxiv.org/abs/2102.07064)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnFocal(nn.Module):\n",
    "    def __init__(self, H, W, req_grad):\n",
    "        super(LearnFocal, self).__init__()\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.fx = nn.Parameter(torch.tensor(1.0, dtype=torch.float32), requires_grad=req_grad)  # (1, )\n",
    "        self.fy = nn.Parameter(torch.tensor(1.0, dtype=torch.float32), requires_grad=req_grad)  # (1, )\n",
    "\n",
    "    def forward(self):\n",
    "        # order = 2, check our supplementary.\n",
    "        fxfy = torch.stack([self.fx**2 * self.W, self.fy**2 * self.H])\n",
    "        return fxfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define learnable POSES\n",
    "\n",
    "Given $N$ input images, we learn a camera pose for each of them. Camera rotations are optimised in axis-angle representation and translations are optimised in Euclidean space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2skew(v):\n",
    "    \"\"\"\n",
    "    :param v:  (3, ) torch tensor\n",
    "    :return:   (3, 3)\n",
    "    \"\"\"\n",
    "    zero = torch.zeros(1, dtype=torch.float32, device=v.device)\n",
    "    skew_v0 = torch.cat([ zero,    -v[2:3],   v[1:2]])  # (3, 1)\n",
    "    skew_v1 = torch.cat([ v[2:3],   zero,    -v[0:1]])\n",
    "    skew_v2 = torch.cat([-v[1:2],   v[0:1],   zero])\n",
    "    skew_v = torch.stack([skew_v0, skew_v1, skew_v2], dim=0)  # (3, 3)\n",
    "    return skew_v  # (3, 3)\n",
    "\n",
    "\n",
    "def Exp(r):\n",
    "    \"\"\"so(3) vector to SO(3) matrix\n",
    "    :param r: (3, ) axis-angle, torch tensor\n",
    "    :return:  (3, 3)\n",
    "    \"\"\"\n",
    "    skew_r = vec2skew(r)  # (3, 3)\n",
    "    norm_r = r.norm() + 1e-15\n",
    "    eye = torch.eye(3, dtype=torch.float32, device=r.device)\n",
    "    R = eye + (torch.sin(norm_r) / norm_r) * skew_r + ((1 - torch.cos(norm_r)) / norm_r**2) * (skew_r @ skew_r)\n",
    "    return R\n",
    "\n",
    "\n",
    "def make_c2w(r, t):\n",
    "    \"\"\"\n",
    "    :param r:  (3, ) axis-angle             torch tensor\n",
    "    :param t:  (3, ) translation vector     torch tensor\n",
    "    :return:   (4, 4)\n",
    "    \"\"\"\n",
    "    R = Exp(r)  # (3, 3)\n",
    "    c2w = torch.cat([R, t.unsqueeze(1)], dim=1)  # (3, 4)\n",
    "    c2w = convert3x4_4x4(c2w)  # (4, 4)\n",
    "    return c2w\n",
    "\n",
    "\n",
    "class LearnPose(nn.Module):\n",
    "    def __init__(self, num_cams, learn_R, learn_t):\n",
    "        super(LearnPose, self).__init__()\n",
    "        self.num_cams = num_cams\n",
    "        self.r = nn.Parameter(torch.zeros(size=(num_cams, 3), dtype=torch.float32), requires_grad=learn_R)  # (N, 3)\n",
    "        self.t = nn.Parameter(torch.zeros(size=(num_cams, 3), dtype=torch.float32), requires_grad=learn_t)  # (N, 3)\n",
    "\n",
    "    def forward(self, cam_id):\n",
    "        r = self.r[cam_id]  # (3, ) axis-angle\n",
    "        t = self.t[cam_id]  # (3, )\n",
    "        c2w = make_c2w(r, t)  # (4, 4)\n",
    "        return c2w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a tiny NeRF\n",
    "We define a tiny NeRF for faster colab training, with following modifications:\n",
    "- We use 4 linear layers (official NeRF has 8) before the RGB and density fully connect layers.\n",
    "- We discard the shortcut since the tiny NeRF is quite shallow now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNerf(nn.Module):\n",
    "    def __init__(self, pos_in_dims, dir_in_dims, D):\n",
    "        \"\"\"\n",
    "        :param pos_in_dims: scalar, number of channels of encoded positions\n",
    "        :param dir_in_dims: scalar, number of channels of encoded directions\n",
    "        :param D:           scalar, number of hidden dimensions\n",
    "        \"\"\"\n",
    "        super(TinyNerf, self).__init__()\n",
    "\n",
    "        self.pos_in_dims = pos_in_dims\n",
    "        self.dir_in_dims = dir_in_dims\n",
    "\n",
    "        self.layers0 = nn.Sequential(\n",
    "            nn.Linear(pos_in_dims, D), nn.ReLU(),\n",
    "            nn.Linear(D, D), nn.ReLU(),\n",
    "            nn.Linear(D, D), nn.ReLU(),\n",
    "            nn.Linear(D, D), nn.ReLU(),\n",
    "            nn.Linear(D, D), nn.ReLU(),\n",
    "            nn.Linear(D, D), nn.ReLU(),\n",
    "            nn.Linear(D, D), nn.ReLU(),\n",
    "            nn.Linear(D, D), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_density = nn.Linear(D, 1) \n",
    "        self.fc_feature = nn.Linear(D, D)\n",
    "        self.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D//2), nn.ReLU())\n",
    "        self.fc_rgb = nn.Linear(D//2, 3) \n",
    "\n",
    "        self.fc_density.bias.data = torch.tensor([0.1]).float()\n",
    "        self.fc_rgb.bias.data = torch.tensor([0.02, 0.02, 0.02]).float()\n",
    "\n",
    "    def forward(self, pos_enc, dir_enc):\n",
    "        \"\"\"\n",
    "        :param pos_enc: (H, W, N_sample, pos_in_dims) encoded positions\n",
    "        :param dir_enc: (H, W, N_sample, dir_in_dims) encoded directions\n",
    "        :return: rgb_density (H, W, N_sample, 4)\n",
    "        \"\"\"\n",
    "        x = self.layers0(pos_enc)  # (H, W, N_sample, D)\n",
    "        density = self.fc_density(x)  # (H, W, N_sample, 1)\n",
    "\n",
    "        feat = self.fc_feature(x)  # (H, W, N_sample, D)\n",
    "        x = torch.cat([feat, dir_enc], dim=3)  # (H, W, N_sample, D+dir_in_dims)\n",
    "        x = self.rgb_layers(x)  # (H, W, N_sample, D/2)\n",
    "        rgb = self.fc_rgb(x)  # (H, W, N_sample, 3)\n",
    "\n",
    "        rgb_den = torch.cat([rgb, density], dim=3)  # (H, W, N_sample, 4)\n",
    "        return rgb_den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set ray parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  class RayParameters():\n",
    "      def __init__(self):\n",
    "        self.NEAR, self.FAR = 0.0, 1.0  # ndc near far\n",
    "        self.N_SAMPLE = 128  # samples per ray\n",
    "        self.POS_ENC_FREQ = 10  # positional encoding freq for location\n",
    "        self.DIR_ENC_FREQ = 4   # positional encoding freq for direction\n",
    "\n",
    "  ray_params = RayParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training function\n",
    "During training, two key steps enable back-propagation:\n",
    "1. Compute ray directions using estimated intrinsics online.\n",
    "2. Sample the 3D volume using estimated poses and intrinsics online.\n",
    "\n",
    "We highlight those two parts in their related comments with \"KEY\" keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_render_image(c2w, rays_cam, t_vals, ray_params, H, W, fxfy, nerf_model,\n",
    "                       perturb_t, sigma_noise_std):\n",
    "    \"\"\"\n",
    "    :param c2w:         (4, 4)                  pose to transform ray direction from cam to world.\n",
    "    :param rays_cam:    (someH, someW, 3)       ray directions in camera coordinate, can be random selected\n",
    "                                                rows and cols, or some full rows, or an entire image.\n",
    "    :param t_vals:      (N_samples)             sample depth along a ray.\n",
    "    :param perturb_t:   True/False              perturb t values.\n",
    "    :param sigma_noise_std: float               add noise to raw density predictions (sigma).\n",
    "    :return:            (someH, someW, 3)       volume rendered images for the input rays.\n",
    "    \"\"\"\n",
    "    # KEY 2: sample the 3D volume using estimated poses and intrinsics online.\n",
    "    # (H, W, N_sample, 3), (H, W, 3), (H, W, N_sam)\n",
    "    sample_pos, _, ray_dir_world, t_vals_noisy = volume_sampling_ndc(c2w, rays_cam, t_vals, ray_params.NEAR,\n",
    "                                                                     ray_params.FAR, H, W, fxfy, perturb_t)\n",
    "\n",
    "    # encode position: (H, W, N_sample, (2L+1)*C = 63)\n",
    "    pos_enc = encode_position(sample_pos, levels=ray_params.POS_ENC_FREQ, inc_input=True)\n",
    "\n",
    "    # encode direction: (H, W, N_sample, (2L+1)*C = 27)\n",
    "    ray_dir_world = F.normalize(ray_dir_world, p=2, dim=2)  # (H, W, 3)\n",
    "    dir_enc = encode_position(ray_dir_world, levels=ray_params.DIR_ENC_FREQ, inc_input=True)  # (H, W, 27)\n",
    "    dir_enc = dir_enc.unsqueeze(2).expand(-1, -1, ray_params.N_SAMPLE, -1)  # (H, W, N_sample, 27)\n",
    "\n",
    "    # inference rgb and density using position and direction encoding.\n",
    "    rgb_density = nerf_model(pos_enc, dir_enc)  # (H, W, N_sample, 4)\n",
    "\n",
    "    render_result = volume_rendering(rgb_density, t_vals_noisy, sigma_noise_std, rgb_act_fn=torch.sigmoid)\n",
    "    rgb_rendered = render_result['rgb']  # (H, W, 3)\n",
    "    depth_map = render_result['depth_map']  # (H, W)\n",
    "\n",
    "    result = {\n",
    "        'rgb': rgb_rendered,  # (H, W, 3)\n",
    "        'depth_map': depth_map,  # (H, W)\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def train_one_epoch(imgs, H, W, ray_params, opt_nerf, opt_focal,\n",
    "                    opt_pose, nerf_model, focal_net, pose_param_net):\n",
    "    nerf_model.train()\n",
    "    focal_net.train()\n",
    "    pose_param_net.train()\n",
    "\n",
    "    t_vals = torch.linspace(ray_params.NEAR, ray_params.FAR, ray_params.N_SAMPLE, device='cuda')  # (N_sample,) sample position\n",
    "    L2_loss_epoch = []\n",
    "\n",
    "    # shuffle the training imgs\n",
    "    ids = np.arange(N_IMGS)\n",
    "    np.random.shuffle(ids)\n",
    "\n",
    "    for i in ids:\n",
    "        fxfy = focal_net()\n",
    "\n",
    "        # KEY 1: compute ray directions using estimated intrinsics online.\n",
    "        ray_dir_cam = comp_ray_dir_cam_fxfy(H, W, fxfy[0], fxfy[1])\n",
    "        img = imgs[i].to('cuda')  # (H, W, 4)\n",
    "        c2w = pose_param_net(i)  # (4, 4)\n",
    "\n",
    "        # sample 32x32 pixel on an image and their rays for training.\n",
    "        r_id = torch.randperm(H, device='cuda')[:32]  # (N_select_rows)\n",
    "        c_id = torch.randperm(W, device='cuda')[:32]  # (N_select_cols)\n",
    "        ray_selected_cam = ray_dir_cam[r_id][:, c_id]  # (N_select_rows, N_select_cols, 3)\n",
    "        img_selected = img[r_id][:, c_id]  # (N_select_rows, N_select_cols, 3)\n",
    "\n",
    "        # render an image using selected rays, pose, sample intervals, and the network\n",
    "        render_result = model_render_image(c2w, ray_selected_cam, t_vals, ray_params,\n",
    "                                           H, W, fxfy, nerf_model, perturb_t=True, sigma_noise_std=0.0)\n",
    "        rgb_rendered = render_result['rgb']  # (N_select_rows, N_select_cols, 3)\n",
    "        L2_loss = F.mse_loss(rgb_rendered, img_selected)  # loss for one image\n",
    "\n",
    "        L2_loss.backward()\n",
    "        opt_nerf.step()\n",
    "        opt_focal.step()\n",
    "        opt_pose.step()\n",
    "        opt_nerf.zero_grad()\n",
    "        opt_focal.zero_grad()\n",
    "        opt_pose.zero_grad()\n",
    "\n",
    "        L2_loss_epoch.append(L2_loss)\n",
    "\n",
    "    L2_loss_epoch_mean = torch.stack(L2_loss_epoch).mean().item()\n",
    "    return L2_loss_epoch_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation function\n",
    "\n",
    "We render an image from a $4\\times4$ identity matrix as we do not have train/eval pose at all.\n",
    "\n",
    "Render results can be found in the tensorboard above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_novel_view(c2w, H, W, fxfy, ray_params, nerf_model):\n",
    "    nerf_model.eval()\n",
    "\n",
    "    ray_dir_cam = comp_ray_dir_cam_fxfy(H, W, fxfy[0], fxfy[1])\n",
    "    t_vals = torch.linspace(ray_params.NEAR, ray_params.FAR, ray_params.N_SAMPLE, device='cuda')  # (N_sample,) sample position\n",
    "\n",
    "    c2w = c2w.to('cuda')  # (4, 4)\n",
    "\n",
    "    # split an image to rows when the input image resolution is high\n",
    "    rays_dir_cam_split_rows = ray_dir_cam.split(10, dim=0)  # input 10 rows each time\n",
    "    rendered_img = []\n",
    "    rendered_depth = []\n",
    "    for rays_dir_rows in rays_dir_cam_split_rows:\n",
    "        render_result = model_render_image(c2w, rays_dir_rows, t_vals, ray_params,\n",
    "                                           H, W, fxfy, nerf_model,\n",
    "                                           perturb_t=False, sigma_noise_std=0.0)\n",
    "        rgb_rendered_rows = render_result['rgb']  # (num_rows_eval_img, W, 3)\n",
    "        depth_map = render_result['depth_map']  # (num_rows_eval_img, W)\n",
    "\n",
    "        rendered_img.append(rgb_rendered_rows)\n",
    "        rendered_depth.append(depth_map)\n",
    "\n",
    "    # combine rows to an image\n",
    "    rendered_img = torch.cat(rendered_img, dim=0)  # (H, W, 3)\n",
    "    rendered_depth = torch.cat(rendered_depth, dim=0)  # (H, W)\n",
    "    return rendered_img, rendered_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring CUDA for Deterministic Execution in Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorborad 📈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# Print the available GPUs and their device numbers\n",
    "for gpu_num in range(num_gpus):\n",
    "    print(f\"GPU {gpu_num}: {torch.cuda.get_device_name(gpu_num)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinstall torch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --force-reinstall torch==1.12.1+cu113 --extra-index-url https://download.pytorch.org/whl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training ⚽\n",
    "- In our paper, we train our our NeRF-- for 10K epochs. but due to the limited GPU resources available in colab, we train our NeRF-- system for 200 epochs (< 10 min) for now, from which a blurry nerf and a rough estimation of camera parameters can be acquired. \n",
    "- Depending on scenes, the depth estimation should start making sense at some point. All examples in this notebook should have a correct depth estimation before 200 epochs. However, we cannot guarentee similar performance for uploaded images.\n",
    "- ***Despite the renderings after 200 epochs are blurry, which can be improved progressively during training, a near-far relationship should be established in 200 epochs for our prepared data in this notebook experiment***. A darker area in a depth map denotes closer distance than a brighter area.\n",
    "- During training, we check if the joint optimisation is successful by writing the depth estimation to tensorboard every 50 epochs. The depth map is estimated from a camera pose that is set to identity matrix. Check the *Image* tab in the tensorboard above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 5000  # set to 1000 to get slightly better results. we use 10K epoch in our paper.\n",
    "EVAL_INTERVAL = 50  # render an image to visualise for every this interval.\n",
    "\n",
    "print('Loading data...')\n",
    "# Initialise all trainabled parameters\n",
    "focal_net = LearnFocal(H, W, req_grad=True).cuda()\n",
    "pose_param_net = LearnPose(num_cams=N_IMGS, learn_R=True, learn_t=True).cuda()\n",
    "\n",
    "# Get a tiny NeRF model. Hidden dimension set to 128\n",
    "nerf_model = TinyNerf(pos_in_dims=63, dir_in_dims=27, D=128).cuda()\n",
    "\n",
    "# Set lr and scheduler: these are just stair-case exponantial decay lr schedulers.\n",
    "opt_nerf = torch.optim.Adam(nerf_model.parameters(), lr=0.001)\n",
    "opt_focal = torch.optim.Adam(focal_net.parameters(), lr=0.001)\n",
    "opt_pose = torch.optim.Adam(pose_param_net.parameters(), lr=0.001)\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "scheduler_nerf = MultiStepLR(opt_nerf, milestones=list(range(0, 10000, 10)), gamma=0.9954)\n",
    "scheduler_focal = MultiStepLR(opt_focal, milestones=list(range(0, 10000, 100)), gamma=0.9)\n",
    "scheduler_pose = MultiStepLR(opt_pose, milestones=list(range(0, 10000, 100)), gamma=0.9)\n",
    "\n",
    "# Set tensorboard writer\n",
    "writer = SummaryWriter(log_dir=os.path.join('logs', scene_name, str(datetime.datetime.now().strftime('%y%m%d_%H%M%S'))))\n",
    "\n",
    "# Store poses to visualise them later\n",
    "pose_history = []\n",
    "\n",
    "# Training\n",
    "print('Training... Check results in the tensorboard above.')\n",
    "for epoch_i in range(N_EPOCH):\n",
    "    L2_loss = train_one_epoch(imgs, H, W, ray_params, opt_nerf, opt_focal,\n",
    "                              opt_pose, nerf_model, focal_net, pose_param_net)\n",
    "    train_psnr = mse2psnr(L2_loss)\n",
    "\n",
    "    writer.add_scalar('train/psnr', train_psnr, epoch_i)\n",
    "\n",
    "    fxfy = focal_net()\n",
    "    print('epoch {0:4d} Training PSNR {1:.3f}, estimated fx {2:.1f} fy {3:.1f}'.format(epoch_i, train_psnr, fxfy[0], fxfy[1]))\n",
    "\n",
    "    scheduler_nerf.step()\n",
    "    scheduler_focal.step()\n",
    "    scheduler_pose.step()\n",
    "\n",
    "    learned_c2ws = torch.stack([pose_param_net(i) for i in range(N_IMGS)])  # (N, 4, 4)\n",
    "    pose_history.append(learned_c2ws[:, :3, 3])  # (N, 3) only store positions as we visualize in 2D.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if (epoch_i+1) % EVAL_INTERVAL == 0:\n",
    "            eval_c2w = torch.eye(4, dtype=torch.float32)  # (4, 4)\n",
    "            fxfy = focal_net()\n",
    "            rendered_img, rendered_depth = render_novel_view(eval_c2w, H, W, fxfy, ray_params, nerf_model)\n",
    "            writer.add_image('eval/img', rendered_img.permute(2, 0, 1), global_step=epoch_i)\n",
    "            writer.add_image('eval/depth', rendered_depth.unsqueeze(0), global_step=epoch_i)\n",
    "\n",
    "pose_history = torch.stack(pose_history).detach().cpu().numpy()  # (N_epoch, N_img, 3)\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pose History', pose_history.__len__())\n",
    "#last 40 value of pose history\n",
    "pose_history = pose_history[-4:]\n",
    "print('Pose History', pose_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel View Synthesis 🖼️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render novel views from a sprial camera trajectory.\n",
    "# The spiral trajectory generation function is modified from https://github.com/kwea123/nerf_pl.\n",
    "from nerfmm.utils.pose_utils import create_spiral_poses\n",
    "\n",
    "# Render full images are time consuming, especially on colab so we render a smaller version instead.\n",
    "resize_ratio = 1\n",
    "with torch.no_grad():\n",
    "    optimised_poses = torch.stack([pose_param_net(i) for i in range(N_IMGS)])\n",
    "    radii = np.percentile(np.abs(optimised_poses.cpu().numpy()[:, :3, 3]), q=50, axis=0)  # (3,)\n",
    "    spiral_c2ws = create_spiral_poses(radii, focus_depth=3.5, n_poses=30, n_circle=1)\n",
    "    spiral_c2ws = torch.from_numpy(spiral_c2ws).float()  # (N, 3, 4)\n",
    "\n",
    "    # change intrinsics according to resize ratio\n",
    "    fxfy = focal_net()\n",
    "    novel_fxfy = fxfy / resize_ratio\n",
    "    novel_H, novel_W = H // resize_ratio, W // resize_ratio\n",
    "\n",
    "    print('NeRF trained in {0:d} x {1:d} for {2:d} epochs'.format(H, W, N_EPOCH))\n",
    "    print('Rendering novel views in {0:d} x {1:d}'.format(novel_H, novel_W))\n",
    "\n",
    "    novel_img_list, novel_depth_list = [], []\n",
    "    for i in range(spiral_c2ws.shape[0]):\n",
    "        novel_img, novel_depth = render_novel_view(spiral_c2ws[i], novel_H, novel_W, novel_fxfy,\n",
    "                                                ray_params, nerf_model)\n",
    "        novel_img_list.append(novel_img)\n",
    "        novel_depth_list.append(novel_depth)\n",
    "        print('Rendering novel view {}/{}'.format(i + 1, spiral_c2ws.shape[0]), end='\\r')\n",
    "\n",
    "    print('Novel view rendering done. Saving to GIF images...')\n",
    "    novel_img_list = (torch.stack(novel_img_list) * 255).cpu().numpy().astype(np.uint8)\n",
    "    novel_depth_list = (torch.stack(novel_depth_list) * 200).cpu().numpy().astype(np.uint8)  # depth is always in 0 to 1 in NDC\n",
    "\n",
    "    # Specify the output folder with\n",
    "    output_folder = 'Output/Output_26_Dataset_5'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    imageio.mimwrite(os.path.join(output_folder, scene_name + '_img.gif'), novel_img_list, duration = 1000/30, loop=0) # loop=0 means infinite loop and duration = 1000/30 means 30 fps\n",
    "    imageio.mimwrite(os.path.join(output_folder, scene_name + '_depth.gif'), novel_depth_list, duration = 1000/30, loop=0) # loop=0 means infinite loop and duration = 1000/30 means 30 fps\n",
    "    print('GIF images saved in folder:', output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(open(os.path.join('Output/Output_26_Dataset_5', scene_name + '_img.gif'), 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(open(os.path.join('Output/Output_26_Dataset_5', scene_name + '_depth.gif'), 'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Camera Poses 📷\n",
    "\n",
    "We visualise the optimisation process of camera poses from Z direction, i.e. on 2D XY-plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6)) # (6, 6) for 1000 epochs, (8, 8) for 10000 epochs\n",
    "ax0 = plt.subplot(111) \n",
    "ax0.set_xlim((-0.5, 0.5))            \n",
    "ax0.set_ylim((-0.5, 0.5))\n",
    "ax0.set_xlabel('X')\n",
    "ax0.set_ylabel('Y')\n",
    "ax0.grid(ls='--', color='0.7')\n",
    "\n",
    "title = ax0.set_title('')\n",
    "traj_line, = ax0.plot([], [], c='blue', ls='-', marker='^', linewidth=0.7, markersize=4)\n",
    "\n",
    "def drawframe(fr_id):\n",
    "    \"\"\"\n",
    "    :param fr_id: frame id\n",
    "    :param poses: (N_img, 3) camera positions\n",
    "    \"\"\"\n",
    "    poses = pose_history[fr_id]\n",
    "    traj_line.set_data(poses[:, 0], poses[:, 1])\n",
    "    title.set_text('epoch {0:4d}'.format(fr_id))\n",
    "    return (traj_line,)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, drawframe, frames=N_EPOCH, interval=100)\n",
    "\n",
    "plt.close(anim._fig)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "\n",
    "def extract_and_save_frames_from_gif(gif_path, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Read the GIF\n",
    "    gif = imageio.mimread(gif_path)\n",
    "    \n",
    "    # Loop through each frame in the gif\n",
    "    for i, frame in enumerate(gif):\n",
    "        # Construct the filename for each frame\n",
    "        frame_filename = f\"frame_{i}.png\"\n",
    "        frame_path = os.path.join(output_folder, frame_filename)\n",
    "\n",
    "        # Save each frame\n",
    "        imageio.imwrite(frame_path, frame)\n",
    "\n",
    "        print(f\"Frame {i} saved as {frame_filename}\")\n",
    "\n",
    "# Usage\n",
    "gif_path = 'Output/Output_9_Dataset_4_Without_Enhancement_10000_epochs/TEM_Data_img.gif'  \n",
    "output_folder = 'Output/Output_28_Dataset_3/frames'  # Replace with your desired output path\n",
    "\n",
    "extract_and_save_frames_from_gif(gif_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python-headless scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyWavelets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def enhance_and_denoise_image(image):\n",
    "    # Apply denoising\n",
    "    denoised_img = cv2.fastNlMeansDenoising(image, None, h=10, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "    # Convert denoised grayscale image to RGB\n",
    "    denoised_rgb = cv2.cvtColor(denoised_img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Enhance sharpness\n",
    "    enhanced_img = cv2.detailEnhance(denoised_rgb, sigma_s=10, sigma_r=0.15)\n",
    "\n",
    "    return enhanced_img\n",
    "\n",
    "def process_images(input_folder, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Get a list of grayscale image files in the folder\n",
    "    grayscale_files = [f for f in os.listdir(input_folder) if f.endswith('.png')]\n",
    "\n",
    "    # Iterate through each grayscale image\n",
    "    for file_name in tqdm(grayscale_files):\n",
    "        # Load grayscale image\n",
    "        grayscale_img = cv2.imread(os.path.join(input_folder, file_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Enhance and denoise the image\n",
    "        enhanced_img = enhance_and_denoise_image(grayscale_img)\n",
    "\n",
    "        # Save the enhanced RGB image as JPG to the output folder\n",
    "        output_file_name = os.path.splitext(file_name)[0] + '_enhanced.jpg'\n",
    "        output_file_path = os.path.join(output_folder, output_file_name)\n",
    "        cv2.imwrite(output_file_path, enhanced_img)\n",
    "\n",
    "        print(f\"Enhanced and denoised image saved to {output_file_path}\")\n",
    "\n",
    "# Usage\n",
    "input_folder = 'Output/Output_28_Dataset_3/frames'  # Replace with your input folder path\n",
    "output_folder = 'Output/Output_28_Dataset_3/frames/frames_output_enhancer'  # Replace with your output folder path\n",
    "\n",
    "process_images(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.restoration import denoise_wavelet, denoise_nl_means, estimate_sigma\n",
    "from skimage.filters import median\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def denoise_images(input_folder, output_base_folder):\n",
    "   \n",
    "    if not os.path.exists(output_base_folder):\n",
    "        os.makedirs(output_base_folder)\n",
    "\n",
    "    \n",
    "    files = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))]\n",
    "\n",
    "    \n",
    "    denoising_methods = {\n",
    "        'wavelet': denoise_wavelet,\n",
    "        'gaussian_blur': lambda img: cv2.GaussianBlur(img, (5, 5), 0),\n",
    "        'median_filter': lambda img: median(img),\n",
    "        'bilateral_filter': lambda img: cv2.bilateralFilter(img, 9, 75, 75),\n",
    "        'non_local_means': lambda img: denoise_nl_means(img, h=1.15 * np.std(img))\n",
    "    }\n",
    "\n",
    "   \n",
    "    for file in tqdm(files):\n",
    "        # Read the image\n",
    "        img_path = os.path.join(input_folder, file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        for method_name, denoising_function in denoising_methods.items():\n",
    "          \n",
    "            denoised_img = denoising_function(img)\n",
    "\n",
    "            # Create a subfolder for the method if it doesn't exist\n",
    "            output_folder = os.path.join(output_base_folder, method_name)\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            # Save the denoised image\n",
    "            denoised_img_path = os.path.join(output_folder, file)\n",
    "            cv2.imwrite(denoised_img_path, (denoised_img * 255).astype('uint8'))\n",
    "\n",
    "            print(f\"Denoised image using {method_name} saved to {denoised_img_path}\")\n",
    "\n",
    "\n",
    "input_folder = 'Output/Output_26_Dataset_5/frames' \n",
    "output_base_folder = 'Output/Output_26_Dataset_5/frames_output'  \n",
    "\n",
    "denoise_images(input_folder, output_base_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load a pre-trained model from TensorFlow Hub\n",
    "model = hub.load('https://tfhub.dev/captain-pool/esrgan-tf2/1')\n",
    "\n",
    "def enhance_image(image_path, save_path):\n",
    "    pil_image = Image.open(image_path).convert('L')\n",
    "    original_size = pil_image.size  \n",
    "    image = np.array(pil_image)\n",
    "    rgb_image = np.stack((image,) * 3, axis=-1)\n",
    "    \n",
    "    # Convert to TensorFlow tensor and preprocess\n",
    "    rgb_image = tf.cast(rgb_image, tf.float32) / 255.0\n",
    "    rgb_image = tf.expand_dims(rgb_image, 0)\n",
    "\n",
    "    # Apply the model\n",
    "    enhanced_image = model(rgb_image)\n",
    "    enhanced_image = tf.squeeze(enhanced_image, 0)\n",
    "\n",
    "    # Resize to original dimensions\n",
    "    resized_enhanced_image = tf.image.resize(enhanced_image, original_size, method=tf.image.ResizeMethod.BICUBIC)\n",
    "\n",
    "    # Convert the output back to grayscale\n",
    "    grayscale_output = tf.image.rgb_to_grayscale(resized_enhanced_image)\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    save_image = tf.keras.preprocessing.image.array_to_img(grayscale_output)\n",
    "    save_image.save(save_path)\n",
    "\n",
    "    return grayscale_output\n",
    "\n",
    "image_path = 'Datasets/RGB/Dataset_4/Janustomo2_-1.04.jpg'  \n",
    "save_path = 'Output/Output_29/enhanced_image.jpg'  \n",
    "enhanced_image = enhance_image(image_path, save_path)\n",
    "\n",
    "# Display the result\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(enhanced_image.numpy().squeeze(), cmap='gray')  \n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSNR Check between the two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_psnr(image1, image2):\n",
    "    mse = np.mean((image1 - image2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def load_image(image_path):\n",
    "    return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "def show_images_with_psnr(image_path1, image_path2):\n",
    "    img1 = load_image(image_path1)\n",
    "    img2 = load_image(image_path2)\n",
    "\n",
    "    psnr_value = calculate_psnr(img1, img2)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Display first image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img1, cmap='gray')\n",
    "    plt.title('First Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display second image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img2, cmap='gray')\n",
    "    plt.title('Second Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"PSNR Value: {psnr_value:.2f} dB\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image_path1 = 'Datasets/BMP/Dataset_4/Janustomo2_-1.04.bmp'\n",
    "image_path2 = 'Output/Output_29/enhanced_image.jpg'\n",
    "\n",
    "show_images_with_psnr(image_path1, image_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "\n",
    "def calculate_psnr(image1, image2):\n",
    "    mse = np.mean((image1 - image2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def calculate_ssim(image1, image2):\n",
    "    ssim_value, _ = ssim(image1, image2, full=True)\n",
    "    return ssim_value\n",
    "\n",
    "def load_image(image_path):\n",
    "    return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "def show_images_with_metrics(image_path1, image_path2, save_dir):\n",
    "    img1 = load_image(image_path1)\n",
    "    img2 = load_image(image_path2)\n",
    "\n",
    "    psnr_value = calculate_psnr(img1, img2)\n",
    "    ssim_value = calculate_ssim(img1, img2)\n",
    "\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "\n",
    "    # Display first image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img1, cmap='gray')\n",
    "    plt.title('Input TEM Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display second image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img2, cmap='gray')\n",
    "    plt.title('Our Result')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"PSNR Value: {psnr_value:.2f} dB, SSIM Value: {ssim_value:.2f}\")\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = os.path.join(save_dir, \"comparison_image.jpg\")\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Image saved to {save_path}\")\n",
    "\n",
    "# Make sure the directory exists where you want to save the images\n",
    "save_directory = 'Comparism Output'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "image_path1 = 'Datasets/JPG/Dataset_7/2725_-4_Rio16_40kX_0174.jpg'\n",
    "image_path2 = 'Output/Output_28_Dataset_3/frames/frames_output_enhancer/frame_0.jpg'\n",
    "\n",
    "show_images_with_metrics(image_path1, image_path2, save_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
