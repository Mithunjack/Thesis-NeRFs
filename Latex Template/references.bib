@article{Frangakis2021,
   abstract = {Cryo-electron tomography is the only technique that can provide sub-nanometer resolved images of cell regions or even whole cells, without the need of labeling or staining methods. Technological advances over the past decade in electron microscope stability, cameras, stage precision and software have resulted in faster acquisition speeds and considerably improved resolution. In pursuit of even better image resolution, researchers seek to reduce noise – a crucial factor affecting the reliability of the tomogram interpretation and ultimately limiting the achieved resolution. Sub-tomogram averaging is the method of choice for reducing noise in repetitive objects. However, when averaging is not applicable, a trade-off between reducing noise and conserving genuine image details must be achieved. Thus, denoising is an important process that improves the interpretability of the tomogram not only directly but also by facilitating other downstream tasks, such as segmentation and 3D visualization. Here, I review contemporary denoising techniques for cryo-electron tomography by taking into account noise-specific properties of both reconstruction and detector noise. The outcomes of different techniques are compared, in order to help researchers select the most appropriate for each dataset and to achieve better and more reliable interpretation of the tomograms.},
   author = {Achilleas S. Frangakis},
   doi = {10.1016/J.JSB.2021.107804},
   issn = {1047-8477},
   issue = {4},
   journal = {Journal of Structural Biology},
   month = {12},
   pages = {107804},
   pmid = {34732363},
   publisher = {Academic Press},
   title = {It’s noisy out there! A review of denoising techniques in cryo-electron tomography},
   volume = {213},
   year = {2021},
}
@article{Joy2008,
   abstract = {Noise is the single most important limiting factor in scanning electron microscopy. Because of the presence of noise, we are forced to operate the SEM to maximize the available beam current and the beam dose (current × time) at the expense of degraded image resolution, increased charging, and more sample damage. Recent developments in high-performance electron guns, aberration correctors, and lenses are all part of an attempt to attain control of the noise while still achieving ever higher levels of resolution. In this chapter, we will examine noise in the SEM, its origin and properties, its measurement, and how the properties of the detectors used for the collection of secondary emission (SE) electrons and backscatter electrons (BSE) signals affect the noise.},
   author = {David C. Joy},
   doi = {10.1007/978-0-387-72972-5_4},
   journal = {Biological Low-Voltage Scanning Electron Microscopy},
   month = {11},
   pages = {129-144},
   publisher = {Springer, New York, NY},
   title = {Noise and Its Effects on the Low-Voltage SEM},
   url = {https://link.springer.com/chapter/10.1007/978-0-387-72972-5_4},
   year = {2008},
}
@article{Frangakis2001,
   abstract = {Electron tomography is a powerful technique capable of giving unique insights into the three-dimensional structural organization of pleomorphic biological objects. However, visualization and interpretation of the resulting volumetric data are hampered by an extremely low signal-to-noise ratio, especially when ice-embedded biological specimens are investigated. Usually, isosurface representation or volume rendering of such data is hindered without any further signal enhancement. We propose a novel technique for noise reduction based on nonlinear anisotropic diffusion. The approach combines efficient noise reduction with excellent signal preservation and is clearly superior to conventional methods (e.g., low-pass and median filtering) and invariant wavelet transform filtering. The gain in the signal-to-noise ratio is verified and demonstrated by means of Fourier shell correlation. Improved visualization performance after processing the 3D images is demonstrated with two examples, tomographic reconstructions of chromatin and of a mitochondrion. Parameter settings and discretization stencils are presented in detail. © 2001 Elsevier Science.},
   author = {Achilleas S. Frangakis and Reiner Hegerl},
   doi = {10.1006/JSBI.2001.4406},
   issn = {1047-8477},
   issue = {3},
   journal = {Journal of structural biology},
   keywords = {A S Frangakis,Anisotropy,Chromatin / chemistry,Chromatin / ultrastructure,Computer-Assisted / methods*,Computer-Assisted / statistics & numerical data,Diffusion,Image Processing,MEDLINE,Macromolecular Substances,Models,Molecular,Molecular Structure,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,Nonlinear Dynamics,PubMed Abstract,R Hegerl,Research Support,Tomography / methods*,Tomography / statistics & numerical data,doi:10.1006/jsbi.2001.4406,pmid:11722164},
   pages = {239-250},
   pmid = {11722164},
   publisher = {J Struct Biol},
   title = {Noise reduction in electron tomographic reconstructions using nonlinear anisotropic diffusion},
   volume = {135},
   url = {https://pubmed.ncbi.nlm.nih.gov/11722164/},
   year = {2001},
}
@article{Fernandez2023,
   abstract = {FlowDenoising is a software tool that implements an adaptive Gaussian denoising filter that preserves visually appreciable structures in volumes of 3D electron microscopy (3DEM). It proceeds by nonrigidly aligning the 2D slices in each dimension, using an optical flow estimator, prior to applying a standard separable (1D) Gaussian filter. FlowDenoising has been developed in Python leveraging well-known public domain libraries, such as OpenCV and NumPy. Furthermore, the software tool exploits data-level parallelism to significantly reduce processing times. Its abilities to denoise huge volumes in just minutes on standard multicore computers makes it a useful tool in 3DEM to explore the interior of cells and tissues at the nanoscale.},
   author = {Vicente González-Ruiz and Jose Jesus Fernández},
   doi = {10.1016/J.SOFTX.2023.101413},
   issn = {2352-7110},
   journal = {SoftwareX},
   keywords = {3D electron microscopy,CryoET,FIB-SEM,Gaussian denoising,Noise filtering,Optical flow},
   month = {7},
   pages = {101413},
   publisher = {Elsevier},
   title = {FlowDenoising: Structure-preserving denoising in 3D electron microscopy (3DEM)},
   volume = {23},
   year = {2023},
}
@article{Mildenhall2020,
   abstract = {We present a method that achieves state-of-the-art results for synthesizing
novel views of complex scenes by optimizing an underlying continuous volumetric
scene function using a sparse set of input views. Our algorithm represents a
scene using a fully-connected (non-convolutional) deep network, whose input is
a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing
direction $(\theta, \phi)$) and whose output is the volume density and
view-dependent emitted radiance at that spatial location. We synthesize views
by querying 5D coordinates along camera rays and use classic volume rendering
techniques to project the output colors and densities into an image. Because
volume rendering is naturally differentiable, the only input required to
optimize our representation is a set of images with known camera poses. We
describe how to effectively optimize neural radiance fields to render
photorealistic novel views of scenes with complicated geometry and appearance,
and demonstrate results that outperform prior work on neural rendering and view
synthesis. View synthesis results are best viewed as videos, so we urge readers
to view our supplementary video for convincing comparisons.},
   author = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
   doi = {10.1007/978-3-030-58452-8_24},
   isbn = {9783030584511},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {3D deep learning,Image-based rendering,Scene representation,View synthesis,Volume rendering},
   month = {3},
   pages = {405-421},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
   volume = {12346 LNCS},
   url = {https://arxiv.org/abs/2003.08934v2},
   year = {2020},
}

@article{Egerton2004,
   abstract = {We review the various ways in which an electron beam can adversely affect an organic or inorganic sample during examination in an electron microscope. The effects considered are: heating, electrostatic charging, ionization damage (radiolysis), displacement damage, sputtering and hydrocarbon contamination. In each case, strategies to minimise the damage are identified. In the light of recent experimental evidence, we re-examine two common assumptions: that the amount of radiation damage is proportional to the electron dose and is independent of beam diameter; and that the extent of the damage is proportional to the amount of energy deposited in the specimen. © 2004 Elsevier Ltd. All rights reserved.},
   author = {R. F. Egerton and P. Li and M. Malac},
   doi = {10.1016/J.MICRON.2004.02.003},
   issn = {0968-4328},
   issue = {6},
   journal = {Micron},
   keywords = {Electron sputtering,Radiation damage,Radiolysis,Scanning electron microscope,Transmission electron microscope},
   month = {8},
   pages = {399-409},
   pmid = {15120123},
   publisher = {Pergamon},
   title = {Radiation damage in the TEM and SEM},
   volume = {35},
   year = {2004},
}
@article{Tang2017,
   abstract = {Transmission electron microscopy (TEM) has been widely applied to characterize morphology, crystalline structure, and elemental information of membrane materials. In this chapter, fundamental knowledge of TEM techniques and their applications in membrane characterization are presented. The two basic modes of TEM, i.e., the bright-field mode and dark-field mode, are introduced and illustrated with TEM micrographs. Crystalline structure and elemental information of specimens can also be obtained. After the introduction of some common membrane sample preparation techniques, the applications of TEM techniques for the detailed characterization of membranes and their building blocks are presented in detail. The application of TEM techniques to characterization the tomography of membrane rejection layer and the morphology of fouling cake layer are also illustrated.},
   author = {C. Y. Tang and Z. Yang},
   doi = {10.1016/B978-0-444-63776-5.00008-5},
   isbn = {9780444637918},
   journal = {Membrane Characterization},
   keywords = {Cross-sections,Fouling cake layer,Membrane characterization,Thin film composite polyamide,Tomography,Transmission electron microscopy},
   month = {1},
   pages = {145-159},
   publisher = {Elsevier},
   title = {Transmission Electron Microscopy (TEM)},
   year = {2017},
}
@article{Loiseau2019,
   abstract = {The localized surface plasmon resonance (LSPR) property of metallic nanoparticles is widely exploited for chemical and biological sensing. Selective biosensing of molecules using functionalized nanoparticles has become a major research interdisciplinary area between chemistry, biology and material science. Noble metals, especially gold (Au) and silver (Ag) nanoparticles, exhibit unique and tunable plasmonic properties; the control over these metal nanostructures size and shape allows manipulating their LSPR and their response to the local environment. In this review, we will focus on Ag-based nanoparticles, a metal that has probably played the most important role in the development of the latest plasmonic applications, owing to its unique properties. We will first browse the methods for AgNPs synthesis allowing for controlled size, uniformity and shape. Ag-based biosensing is often performed with coated particles; therefore, in a second part, we will explore various coating strategies (organics, polymers, and inorganics) and their influence on coated-AgNPs properties. The third part will be devoted to the combination of gold and silver for plasmonic biosensing, in particular the use of mixed Ag and AuNPs, i.e., AgAu alloys or Ag-Au core@shell nanoparticles will be outlined. In the last part, selected examples of Ag and AgAu-based plasmonic biosensors will be presented.},
   author = {Alexis Loiseau and Victoire Asila and Gabriel Boitel-Aullen and Mylan Lam and Michèle Salmain and Souhir Boujday},
   doi = {10.3390/bios9020078},
   keywords = {LSPR,alloy,biosensors,coating,core@shell,silver nanoparticles,synthesis},
   title = {Silver-Based Plasmonic Nanoparticles for and Their Use in Biosensing},
   url = {www.mdpi.com/journal/biosensors},
   year = {2019},
}

@article{Gault2008,
   abstract = {The application of wide field-of-view detection systems to atom probe experiments emphasizes the importance of careful parameter selection in the tomographic reconstruction of the analyzed volume, as the sensitivity to errors rises steeply with increases in analysis dimensions. In this article, a self-consistent method is presented for the systematic determination of the main reconstruction parameters. In the proposed approach, the compression factor and the field factor are determined using geometrical projections from the desorption images. A three-dimensional Fourier transform is then applied to a series of reconstructions, and after comparing to the known material crystallography, the efficiency of the detector is estimated. The final results demonstrate a significant improvement in the accuracy of the reconstructed volumes. Copyright © Microscopy Society of America 2008.},
   author = {Baptiste Gault and Frederic De Geuser and Leigh T. Stephenson and Michael P. Moody and Barrington C. Muddle and Simon P. Ringer},
   doi = {10.1017/S1431927608080690},
   issn = {1431-9276},
   issue = {4},
   journal = {Microscopy and Microanalysis},
   keywords = {Atom probe tomography (APT),Data analysis,Field desorption microscopy,Fourier analysis,Spatial resolution,Three-dimensional reconstruction},
   month = {8},
   pages = {296-305},
   publisher = {Oxford Academic},
   title = {Estimation of the Reconstruction Parameters for Atom Probe Tomography},
   volume = {14},
   url = {https://dx.doi.org/10.1017/S1431927608080690},
   year = {2008},
}
@article{Adrian1984,
   abstract = {Thin vitrified layers of unfixed, unstained and unsupported virus suspensions can be prepared for observation by cryo-electron microscopy in easily controlled conditions. The viral particles appear free from the kind of damage caused by dehydration, freezing or adsorption to a support that is encountered in preparing biological samples for conventional electron microscopy. Cryo-electron microscopy of vitrified specimens offers possibilities for high resolution observations that compare favourably with any other electron microscopical method.},
   author = {Marc Adrian and Jacques Dubochet and Jean Lepault and Alasdair W. McDowall},
   doi = {10.1038/308032a0},
   issn = {1476-4687},
   issue = {5954},
   journal = {Nature 1984 308:5954},
   keywords = {Humanities and Social Sciences,Science,multidisciplinary},
   pages = {32-36},
   pmid = {6322001},
   publisher = {Nature Publishing Group},
   title = {Cryo-electron microscopy of viruses},
   volume = {308},
   url = {https://www.nature.com/articles/308032a0},
   year = {1984},
}
@article{Park2019,
   abstract = {Computer graphics, 3D computer vision and robotics communities have produced
multiple approaches to representing 3D geometry for rendering and
reconstruction. These provide trade-offs across fidelity, efficiency and
compression capabilities. In this work, we introduce DeepSDF, a learned
continuous Signed Distance Function (SDF) representation of a class of shapes
that enables high quality shape representation, interpolation and completion
from partial and noisy 3D input data. DeepSDF, like its classical counterpart,
represents a shape's surface by a continuous volumetric field: the magnitude of
a point in the field represents the distance to the surface boundary and the
sign indicates whether the region is inside (-) or outside (+) of the shape,
hence our representation implicitly encodes a shape's boundary as the
zero-level-set of the learned function while explicitly representing the
classification of space as being part of the shapes interior or not. While
classical SDF's both in analytical or discretized voxel form typically
represent the surface of a single shape, DeepSDF can represent an entire class
of shapes. Furthermore, we show state-of-the-art performance for learned 3D
shape representation and completion while reducing the model size by an order
of magnitude compared with previous work.},
   author = {Jeong Joon Park and Peter Florence and Julian Straub and Richard Newcombe and Steven Lovegrove},
   doi = {10.1109/CVPR.2019.00025},
   isbn = {9781728132938},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   keywords = {3D from Multiview and Sensors,Deep Learning,Representation Learning,Vision + Graphics},
   month = {1},
   pages = {165-174},
   publisher = {IEEE Computer Society},
   title = {DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation},
   volume = {2019-June},
   url = {https://arxiv.org/abs/1901.05103v1},
   year = {2019},
}
@article{Mescheder2018,
   abstract = {With the advent of deep neural networks, learning-based approaches for 3D
reconstruction have gained popularity. However, unlike for images, in 3D there
is no canonical representation which is both computationally and memory
efficient yet allows for representing high-resolution geometry of arbitrary
topology. Many of the state-of-the-art learning-based 3D reconstruction
approaches can hence only represent very coarse 3D geometry or are limited to a
restricted domain. In this paper, we propose Occupancy Networks, a new
representation for learning-based 3D reconstruction methods. Occupancy networks
implicitly represent the 3D surface as the continuous decision boundary of a
deep neural network classifier. In contrast to existing approaches, our
representation encodes a description of the 3D output at infinite resolution
without excessive memory footprint. We validate that our representation can
efficiently encode 3D structure and can be inferred from various kinds of
input. Our experiments demonstrate competitive results, both qualitatively and
quantitatively, for the challenging tasks of 3D reconstruction from single
images, noisy point clouds and coarse discrete voxel grids. We believe that
occupancy networks will become a useful tool in a wide variety of
learning-based 3D tasks.},
   author = {Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger},
   doi = {10.1109/CVPR.2019.00459},
   isbn = {9781728132938},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   keywords = {3D from Single Image,Deep Learning},
   month = {12},
   pages = {4455-4465},
   publisher = {IEEE Computer Society},
   title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
   volume = {2019-June},
   url = {https://arxiv.org/abs/1812.03828v2},
   year = {2018},
}
@article{Sitzmann2020,
   abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail. They also fail to accurately model spatial and temporal derivatives, which is necessary to represent signals defined implicitly by differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, three-dimensional shapes, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions. Please see the project website for a video overview of the proposed method and all applications.},
   author = {Vincent Sitzmann and Julien N P Martel and Alexander W Bergman and David B Lindell and Gordon Wetzstein},
   journal = {Advances in Neural Information Processing Systems},
   pages = {7462-7473},
   title = {Implicit Neural Representations with Periodic Activation Functions},
   volume = {33},
   year = {2020},
}
@article{Wu2015,
   abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
   author = {Zhirong Wu and Shuran Song and Aditya Khosla and Fisher Yu and Linguang Zhang and Xiaoou Tang and Jianxiong Xiao},
   doi = {10.1109/CVPR.2015.7298801},
   isbn = {9781467369640},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   month = {10},
   pages = {1912-1920},
   publisher = {IEEE Computer Society},
   title = {3D ShapeNets: A deep representation for volumetric shapes},
   volume = {07-12-June-2015},
   year = {2015},
}
@article{Xia,
   abstract = {Recent years have seen remarkable progress in deep learning powered visual content creation. This includes 3D-aware generative image synthesis, which produces high-fidelity images in a 3D-consistent manner while simultaneously capturing compact surfaces of objects from pure image collections without the need for any 3D supervision, thus bridging the gap between 2D imagery and 3D reality. The 3D-aware generative models have shown that the introduction of 3D information can lead to more controllable image generation. The task of 3D-aware image synthesis has taken the field of computer vision by storm, with hundreds of papers accepted to top-tier journals and conferences in recent year (mainly the past two years), but there lacks a comprehensive survey of this remarkable and swift progress. Our survey aims to introduce new researchers to this topic, provide a useful reference for related works, and stimulate future research directions through our discussion section. Apart from the presented papers, we aim to constantly update the latest relevant papers along with corresponding implementations at https://weihaox.github.io/awesome-3D-aware-synthesis.},
   author = {Weihao Xia and Jing-Hao Xue},
   keywords = {Index Terms-Deep learning,computer vision,generative models,implicit neural representation,novel view synthesis !},
   title = {A Survey on 3D-aware Image Synthesis},
   url = {https://weihaox.github.},
}
@article{Seitz2006,
   abstract = {This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multi-view image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/ mview. © 2006 IEEE.},
   author = {Steven M. Seitz and Brian Curless and James Diebel and Daniel Scharstein and Richard Szeliski},
   doi = {10.1109/CVPR.2006.19},
   isbn = {0769525970},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {519-526},
   title = {A comparison and evaluation of multi-view stereo reconstruction algorithms},
   volume = {1},
   year = {2006},
}
@article{Chen2023,
   abstract = {Image-space simplifications have been used to accelerate the calculation of computer graphic images since the dawn of visual simulation. Texture mapping has been used to provide a means by which images may themselves be used as display primitives. The work reported by this paper endeavors to carry this concept to its logical extreme by using interpolated images to portray three-dimensional scenes. The special-effects technique of morphing, which combines interpolation of texture maps and their shape, is applied to computing arbitrary intermediate frames from an array of prestored images. If the images are a structured set of views of a 3D object or scene, intermediate frames derived by morphing can be used to approximate intermediate 3D transformations of the object or scene. Using the view interpolation approach to synthesize 3D scenes has two main advantages. First, the 3D representation of the scene may be replaced with images. Second, the image synthesis time is independent of the scene complexity. The correspondence between images, required for the morphing method, can be predetermined automatically using the range data associated with the images. The method is further accelerated by a quadtree decomposition and a view-independent visible priority. Our experiments have shown that the morphing can be performed at interactive rates on today's high-end personal computers. Potential applications of the method include virtual holograms, a walkthrough in a virtual environment, image-based primitives and incremental rendering. The method also can be used to greatly accelerate the computation of motion blur and soft shadows cast by area light sources.},
   author = {Shenchang Eric Chen and Lance Williams},
   city = {New York, NY, USA},
   doi = {10.1145/3596711.3596757},
   journal = {Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
   keywords = {CR Categories and Subject Descriptors: I33 [Computer Graphics]: Picture/Image Generation; I37 [Computer Graphics]: Three-Dimensional Graphics and Real-ism Additional Keywords: image morphing,incremental rendering,interpolation,motion blur,motion compensation,real-time display,shadow,virtual holography,virtual reality},
   month = {8},
   pages = {423-432},
   publisher = {ACM},
   title = {View Interpolation for Image Synthesis},
   url = {https://dl.acm.org/doi/10.1145/3596711.3596757},
   year = {2023},
}
@article{Fang,
   abstract = {Figure 1: We propose a radiance field framework equipped with time-aware neural voxels, which can learn dynamic scenes with an extremely fast convergence speed. Comparisons with D-NeRF [Pumarola et al. 2021] are shown. Sparse time-view images are taken and novel time and view images can be synthesized with our method. ABSTRACT Neural radiance fields (NeRF) have shown great success in model-ing 3D scenes and synthesizing novel-view images. However, most previous NeRF methods take much time to optimize one single * Equal contributions. † Corresponding author. scene. Explicit data structures, e.g. voxel features, show great potential to accelerate the training process. However, voxel features face two big challenges to be applied to dynamic scenes, i.e. mod-eling temporal information and capturing different scales of point motions. We propose a radiance field framework by representing scenes with time-aware voxel features, named as TiNeuVox. A tiny coordinate deformation network is introduced to model coarse motion trajectories and temporal information is further enhanced in the radiance network. A multi-distance interpolation method is proposed and applied on voxel features to model both small and large motions. Our framework significantly accelerates the optimization of dynamic radiance fields while maintaining high rendering quality. Empirical evaluation is performed on both synthetic and real scenes. Our TiNeuVox completes training with only 8 minutes and SA '22 Conference Papers, December 6-9, 2022, Daegu, Republic of Korea Fang, Yi, et al. 8-MB storage cost while showing similar or even better rendering performance than previous dynamic NeRF methods. Code is available at https://github.com/hustvl/TiNeuVox. CCS CONCEPTS • Computing methodologies → 3D imaging; Computational photography; Image-based rendering.},
   author = {Jiemin Fang and Taoran Yi and Xinggang Wang and Lingxi Xie and Xiaopeng Zhang and Wenyu Liu and Matthias Nießner and Qi Tian and Qi 2022 Tian},
   doi = {10.1145/3550469.3555383},
   isbn = {9781450394703},
   keywords = {dynamic scenes,neural rendering,neural voxels,novel view synthesis,temporal information encoding},
   title = {Fast Dynamic Radiance Fields with Time-Aware Neural Voxels TiNeuVox (ours) 1 min 4 mins 8 mins Sparse Time-View Input Images Time Synthesis View Synthesis Fast Training for Time-View Synthesis D-NeRF Training Time},
   url = {https://doi.org/10.1145/3550469.3555383},
}
@article{Hartley2000,
   author = {Richard Hartley and Andrew Zisserman},
   isbn = {9780521540513},
   title = {Multiple View Geometry in Computer Vision, Second Edition},
   url = {www.cambridge.org/9780521540513},
   year = {2000},
}
@article{Heikkila1997,
   abstract = {In geometrical camera calibration the objective is to determine a set of camera parameters that describe the mapping between 3-D reference coordinates and 2-D image coordinates. Various methods for camera calibration can be found from the literature. However, surprisingly little attention has been paid to the whole calibration procedure, i.e., control point extraction from images, model fitting, image correction, and errors originating in these stages. The main interest has been in model fitting, although the other stages are also important. In this paper we present a four-step calibration procedure that is an extension to the two-step method. There is an additional step to compensate for distortion caused by circular features, and a step for correcting the distorted image coordinates. The image correction is performed with an empirical inverse model that accurately compensates for radial and tangential distortions. Finally, a linear method for solving the parameters of the inverse model is presented.},
   author = {Janne Heikkila and Olli Silven},
   doi = {10.1109/CVPR.1997.609468},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {1106-1112},
   publisher = {IEEE},
   title = {Four-step camera calibration procedure with implicit image correction},
   year = {1997},
}
@article{Zhang2000,
   abstract = {We propose a flexible new technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use. The corresponding software is available from the author's Web page. © 2000 IEEE.},
   author = {Zhengyou Zhang},
   doi = {10.1109/34.888718},
   issn = {01628828},
   issue = {11},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {2d pattern,Absolute conic,Calibration from planes,Camera calibration,Closed-form solution,Flexible plane-based calibration,Flexible setup,Lens distortion,Maximum likelihood estimation,Projective mapping},
   month = {11},
   pages = {1330-1334},
   title = {A flexible new technique for camera calibration},
   volume = {22},
   year = {2000},
}
@article{Lepetit2009,
   abstract = {We propose a non-iterative solution to the PnP problem-the estimation of the pose of a calibrated camera from n 3D-to-2D point correspondences-whose computational complexity grows linearly with n. This is in contrast to state-of-the-art methods that are O(n 5) or even O(n 8), without being more accurate. Our method is applicable for all n ≥ 4 and handles properly both planar and non-planar configurations. Our central idea is to express the n 3D points as a weighted sum of four virtual control points. The problem then reduces to estimating the coordinates of these control points in the camera referential, which can be done in O(n) time by expressing these coordinates as weighted sum of the eigenvectors of a 12 × 12 matrix and solving a small constant number of quadratic equations to pick the right weights. Furthermore, if maximal precision is required, the output of the closed-form solution can be used to initialize a Gauss-Newton scheme, which improves accuracy with negligible amount of additional time. The advantages of our method are demonstrated by thorough testing on both synthetic and real-data.},
   author = {Vincent Lepetit and Francesc Moreno-Noguer and Pascal Fua},
   doi = {10.1007/S11263-008-0152-6/METRICS},
   issn = {09205691},
   issue = {2},
   journal = {International Journal of Computer Vision},
   keywords = {Absolute orientation,Perspective-n-Point,Pose estimation},
   month = {2},
   pages = {155-166},
   publisher = {Springer},
   title = {EPnP: An accurate O(n) solution to the PnP problem},
   volume = {81},
   url = {https://link.springer.com/article/10.1007/s11263-008-0152-6},
   year = {2009},
}
@article{Zhang1995,
   abstract = {This paper proposes a robust approach to image matching by exploiting the only available geometric constraint, namely, the epipolar constraint. The images are uncalibrated, namely the motion between them and the camera parameters are not known. Thus, the images can be taken by different cameras or a single camera at different time instants. If we make an exhaustive search for the epipolar geometry, the complexity is prohibitively high. The idea underlying our approach is to use classical techniques (correlation and relaxation methods in our particular implementation) to find an initial set of matches, and then use a robust technique-the Least Median of Squares (LMedS)-to discard false matches in this set. The epipolar geometry can then be accurately estimated using a meaningful image criterion. More matches are eventually found, as in stereo matching, by using the recovered epipolar geometry. A large number of experiments have been carried out, and very good results have been obtained. Regarding the relaxation technique, we define a new measure of matching support, which allows a higher tolerance to deformation with respect to rigid transformations in the image plane and a smaller contribution for distant matches than for nearby ones. A new strategy for updating matches is developed, which only selects those matches having both high matching support and low matching ambiguity. The update strategy is different from the classical "winner-take-all", which is easily stuck at a local minimum, and also from "loser-take-nothing", which is usually very slow. The proposed algorithm has been widely tested and works remarkably well in a scene with many repetitive patterns. © 1995.},
   author = {Zhengyou Zhang and Rachid Deriche and Olivier Faugeras and Quang Tuan Luong},
   doi = {10.1016/0004-3702(95)00022-4},
   issn = {0004-3702},
   issue = {1-2},
   journal = {Artificial Intelligence},
   keywords = {Correlation,Epipolar geometry,Fundamental matrix,Least Median of Squares (LMedS),Relaxation,Robust matching},
   month = {10},
   pages = {87-119},
   publisher = {Elsevier},
   title = {A robust technique for matching two uncalibrated images through the recovery of the unknown epipolar geometry},
   volume = {78},
   year = {1995},
}
@article{Lowe1999,
   abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.},
   author = {David G. Lowe},
   doi = {10.1109/ICCV.1999.790410},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   pages = {1150-1157},
   publisher = {IEEE},
   title = {Object recognition from local scale-invariant features},
   volume = {2},
   year = {1999},
}
@article{Fischler1981,
   abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gro...},
   author = {Martin A. Fischler and Robert C. Bolles},
   doi = {10.1145/358669.358692},
   issn = {15577317},
   issue = {6},
   journal = {Communications of the ACM},
   keywords = {automated cartography,automated cartography CR Categories: 360,camera calibration,image matching,location determination,model fitting,scene analysis},
   month = {6},
   pages = {381-395},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Random sample consensus},
   volume = {24},
   url = {https://dl.acm.org/doi/10.1145/358669.358692},
   year = {1981},
}
@article{Snavely,
   abstract = {(c) Figure 1: Our system takes unstructured collections of photographs such as those from online image searches (a) and reconstructs 3D points and viewpoints (b) to enable novel ways of browsing the photos (c). Abstract We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.},
   author = {Noah Snavely and Steven M Seitz and Richard Szeliski and Microsoft Research},
   keywords = {CR Categories: H51 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial,and vir-tual realities I210 [Artificial Intelligence]: Vision and Scene Understanding-Modeling and recovery of physical attributes Keywords: image-based rendering,augmented,image-based modeling,photo browsing,structure from motion},
   title = {Photo Tourism: Exploring Photo Collections in 3D},
   url = {www.cc.gatech.edu/4d-cities},
}
@article{Heinly,
   abstract = {We propose a novel, large-scale, structure-from-motion framework that advances the state of the art in data scal-ability from city-scale modeling (millions of images) to world-scale modeling (several tens of millions of images) using just a single computer. The main enabling technology is the use of a streaming-based framework for connected component discovery. Moreover, our system employs an adaptive, online, iconic image clustering approach based on an augmented bag-of-words representation, in order to balance the goals of registration, comprehensiveness, and data compactness. We demonstrate our proposal by operating on a recent publicly available 100 million image crowd-sourced photo collection containing images geographically distributed throughout the entire world. Results illustrate that our streaming-based approach does not compromise model completeness, but achieves unprecedented levels of efficiency and scalability.},
   author = {Jared Heinly and Johannes L Schönberger and Enrique Dunn and Jan-Michael Frahm},
   title = {Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset)},
}
@article{Triggs,
   abstract = {This paper is a survey of the theory and methods of photogrammetric bundle adjustment, aimed at potential implementors in the computer vision community. Bundle adjustment is the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates. Topics covered include: the choice of cost function and robustness; numerical optimization including sparse Newton methods, linearly convergent approximations, updating and recursive methods; gauge (datum) invariance; and quality control. The theory is developed for general robust cost functions rather than restricting attention to traditional nonlinear least squares.},
   author = {Bill Triggs and Philip Mclauchlan and Richard Hartley and Andrew Fitzgibbon},
   keywords = {Bundle Adjustment,Gauge Freedom,Opti-mization,Scene Reconstruction,Sparse Matrices},
   title = {Bundle Adjustment-A Modern Synthesis},
}
@article{Facciolo2015,
   abstract = {Semi-global matching (SGM) is among the top-ranked stereovision algorithms. SGM is an efficient strategy for approximately minimizing a global energy that comprises a pixel-wise matching cost and pair-wise smoothness terms. In SGM the two-dimensional smoothness constraint is approximated as the average of one-dimensional line optimiza-tion problems. The accuracy and speed of SGM are the main reasons for its widespread adoption, even when applied to generic problems beyond stereovision. This approximate minimization, however, also produces characteristic low amplitude streaks in the final disparity image, and is clearly suboptimal with respect to more comprehensive mini-mization strategies. Based on a recently proposed interpretation of SGM as a min-sum Belief Propagation algorithm, we propose a new algorithm that allows to reduce by a factor five the energy gap of SGM with respect to reference algorithms for MRFs with truncated smoothness terms. The proposed method comes with no compromises with respect to the baseline SGM, no parameters and virtually no computational overhead. At the same time it attains higher quality results by removing the characteristic streaking artifacts of SGM.},
   author = {Gabriele Facciolo and Carlo de Franchis and Enric Meinhardt},
   doi = {10.5244/C.29.90},
   month = {12},
   pages = {90.1-90.12},
   publisher = {British Machine Vision Association and Society for Pattern Recognition},
   title = {MGM: A Significantly More Global Matching for Stereovision},
   year = {2015},
}
@article{Kazhdan,
   abstract = {Poisson surface reconstruction creates watertight surfaces from oriented point sets. In this work we extend the technique to explicitly incorporate the points as interpolation constraints. The extension can be interpreted as a generalization of the underlying mathematical framework to a screened Poisson equation. In contrast to other image and geometry processing techniques, the screening term is defined over a sparse set of points rather than over the full domain. We show that these sparse constraints can nonetheless be integrated efficiently. Because the modified linear system retains the same finite-element discretization, the sparsity structure is unchanged, and the system can still be solved using a multigrid approach. Moreover we present several algorithmic improvements that together reduce the time complexity of the solver to linear in the number of points, thereby enabling faster, higher-quality surface reconstructions.},
   author = {Michael Kazhdan and Johns Hopkins University and Hugues Hoppe},
   doi = {10.1145/XXXXXXX.YYYYYYY},
   journal = {ACM Trans. Graph. NN, N, Article NN (Month YYYY)},
   keywords = {H Screened Poisson surface reconstruction,I35 [Computer Graphics]: Compu-tational Geometry and Object Modeling Additional Key Words and Phrases: screened Poisson equation,M,adaptive octree,and Hoppe,finite elements,surface fitting ACM Reference Format: Kazhdan},
   title = {Screened Poisson Surface Reconstruction},
   url = {http://doi.acm.org/10.1145/XXXXXXX.YYYYYYY},
}
@article{Zhou,
   abstract = {Input Optimized reconstruction Figure 1: Given a geometric model and corresponding color images produced by a consumer-grade RGB-D camera (left), our approach optimizes a photometrically consistent mapping of the images to the model. Abstract We present a global optimization approach for mapping color images onto geometric reconstructions. Range and color videos produced by consumer-grade RGB-D cameras suffer from noise and optical distortions, which impede accurate mapping of the acquired color data to the reconstructed geometry. Our approach addresses these sources of error by optimizing camera poses in tandem with non-rigid correction functions for all images. All parameters are optimized jointly to maximize the photometric consistency of the reconstructed mapping. We show that this optimization can be performed efficiently by an alternating optimization algorithm that in-terleaves analytical updates of the color map with decoupled parameter updates for all images. Experimental results demonstrate that our approach substantially improves color mapping fidelity.},
   author = {Qian-Yi Zhou and Vladlen Koltun},
   keywords = {CR Categories: I35 [Computer Graphics]: Computational Ge-ometry and Object Modeling Keywords: 3d reconstruction,consumer depth cameras,optimization Links: DL PDF * Stanford University † Adobe Research,range imaging,texture mapping},
   title = {Color Map Optimization for 3D Reconstruction with Consumer Depth Cameras},
}
@article{Schönberger,
   abstract = {Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy , completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.},
   author = {Johannes L Schönberger and Jan-Michael Frahm},
   title = {Structure-from-Motion Revisited},
   url = {https://github.com/colmap/colmap.},
}
@article{Lowe2004,
   abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
   author = {David G. Lowe},
   doi = {10.1023/B:VISI.0000029664.99615.94/METRICS},
   issn = {09205691},
   issue = {2},
   journal = {International Journal of Computer Vision},
   keywords = {Image matching,Invariant features,Object recognition,Scale invariance},
   month = {11},
   pages = {91-110},
   publisher = {Springer},
   title = {Distinctive image features from scale-invariant keypoints},
   volume = {60},
   url = {https://link.springer.com/article/10.1023/B:VISI.0000029664.99615.94},
   year = {2004},
}
@article{Galliani,
   abstract = {We present a new, massively parallel method for high-quality multiview matching. Our work builds on the Patch-match idea: starting from randomly generated 3D planes in scene space, the best-fitting planes are iteratively propagated and refined to obtain a 3D depth and normal field per view, such that a robust photo-consistency measure over all images is maximized. Our main novelties are on the one hand to formulate Patchmatch in scene space, which makes it possible to aggregate image similarity across multiple views and obtain more accurate depth maps. And on the other hand a modified, diffusion-like propagation scheme that can be massively parallelized and delivers dense mul-tiview correspondence over ten 1.9-Megapixel images in 3 seconds, on a consumer-grade GPU. Our method uses a slanted support window and thus has no fronto-parallel bias; it is completely local and parallel, such that computation time scales linearly with image size, and inversely proportional to the number of parallel threads. Furthermore , it has low memory footprint (four values per pixel, independent of the depth range). It therefore scales exceptionally well and can handle multiple large images at high depth resolution. Experiments on the DTU and Middlebury multiview datasets as well as oblique aerial images show that our method achieves very competitive results with high accuracy and completeness, across a range of different scenarios .},
   author = {Silvano Galliani and Katrin Lasinger and Konrad Schindler Photogrammetry and Remote Sensing and Eth Zurich},
   title = {Massively Parallel Multiview Stereopsis by Surface Normal Diffusion},
}
@article{Lin2020,
   abstract = {Atom segmentation and localization, noise reduction and deblurring of atomic-resolution scanning transmission electron microscopy (STEM) images with high precision and robustness is a challenging task. Although several conventional algorithms, such has thresholding, edge detection and clustering, can achieve reasonable performance in some predefined sceneries, they tend to fail when interferences from the background are strong and unpredictable. Particularly, for atomic-resolution STEM images, so far there is no well-established algorithm that is robust enough to segment or detect all atomic columns when there is large thickness variation in a recorded image. Herein, we report the development of a training library and a deep learning method that can perform robust and precise atom segmentation, localization, denoising, and super-resolution processing of experimental images. Despite using simulated images as training datasets, the deep-learning model can self-adapt to experimental STEM images and shows outstanding performance in atom detection and localization in challenging contrast conditions and the precision consistently outperforms the state-of-the-art two-dimensional Gaussian fit method. Taking a step further, we have deployed our deep-learning models to a desktop app with a graphical user interface and the app is free and open-source. We have also built a TEM ImageNet project website for easy browsing and downloading of the training data.},
   author = {Ruoqian Lin and Rui Zhang and Chunyang Wang and Xiao-Qing Yang and Huolin L. Xin},
   keywords = {ADF-STEM,atom localization,atomic resolution,convolutional neural networks,deep learning,picometer precision},
   month = {12},
   title = {TEMImageNet Training Library and AtomSegNet Deep-Learning Models for High-Precision Atom Segmentation, Localization, Denoising, and Super-Resolution Processing of Atomic-Resolution Images},
   url = {https://arxiv.org/abs/2012.09093v2},
   year = {2020},
}
@article{Widmer2013,
   abstract = {Analysis of microscopy images can provide insight into many biological processes. One particularly challenging problem is cell nuclear segmentation in highly anisotropic and noisy 3D image data. Manually localizing and segmenting each and every cell nuclei is very time consuming, which remains a bottleneck in large scale biological experiments. In this work we present a tool for automated segmentation of cell nuclei from 3D fluorescent microscopic data. Our tool is based on state-of-the-art image processing and machine learning techniques and supports a friendly graphical user interface (GUI). We show that our tool is as accurate as manual annotation but greatly reduces the time for the registration.},
   author = {Christian Widmer and Stephanie Heinrich and Philipp Drewe and Xinghua Lou and Shefali Umrania and Gunnar Rätsch},
   doi = {10.1007/s11760-014-0694-8},
   issn = {18631711},
   issue = {1},
   journal = {Signal, Image and Video Processing},
   keywords = {3D fluorescent microscopic data,Cell nuclei detection,Nuclear segmentation,Shape reconstruction},
   month = {9},
   pages = {41-48},
   publisher = {Springer-Verlag London Ltd},
   title = {GRED: Graph-Regularized 3D Shape Reconstruction from Highly Anisotropic and Noisy Images},
   volume = {8},
   url = {https://arxiv.org/abs/1309.4426v1},
   year = {2013},
}
@article{Sorzano2004,
   abstract = {Three-dimensional electron microscopy (3D-EM) is a powerful tool for visualizing complex biological systems. As with any other imaging device, the electron microscope introduces a transfer function (called in this field the contrast transfer function, CTF) into the image acquisition process that modulates the various frequencies of the signal. Thus, the 3D reconstructions performed with these CTF-affected projections are also affected by an implicit 3D transfer function. For high-resolution electron microscopy, the effect of the CTF is quite dramatic and limits severely the achievable resolution. In this work we make use of the iterative data refinement (IDR) technique to ameliorate the effect of the CTF. It is demonstrated that the approach can be successfully applied to noisy data. © 2004 IOP Publishing Ltd.},
   author = {C. O.S. Sorzano and R. Marabini and G. T. Herman and Y. Censor and J. M. Carazo},
   doi = {10.1088/0031-9155/49/4/003},
   issn = {0031-9155},
   issue = {4},
   journal = {Physics in medicine and biology},
   keywords = {Algorithms,Bacteriorhodopsins / chemistry*,C O S Sorzano,Computer-Assisted*,Electron / methods*,Fourier Analysis,Image Processing,Imaging,J M Carazo,MEDLINE,Microscopy,Models,Molecular,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,P.H.S.,PubMed Abstract,R Marabini,Research Support,Software*,Three-Dimensional / methods*,U.S. Gov't,doi:10.1088/0031-9155/49/4/003,pmid:15005161},
   month = {2},
   pages = {509-522},
   pmid = {15005161},
   publisher = {Phys Med Biol},
   title = {Transfer function restoration in 3D electron microscopy via iterative data refinement},
   volume = {49},
   url = {https://pubmed.ncbi.nlm.nih.gov/15005161/},
   year = {2004},
}
@article{Zhang2017,
   abstract = {Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance; in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications.},
   author = {Kai Zhang and Wangmeng Zuo and Shuhang Gu and Lei Zhang},
   doi = {10.1109/CVPR.2017.300},
   isbn = {9781538604571},
   journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   month = {4},
   pages = {2808-2817},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Learning Deep CNN Denoiser Prior for Image Restoration},
   volume = {2017-January},
   url = {https://arxiv.org/abs/1704.03264v1},
   year = {2017},
}
@article{Fernandez2012,
   abstract = {Electron tomography (ET) has emerged as a powerful technique to address fundamental questions in molecular and cellular biology. It makes possible visualization of the molecular architecture of complex viruses, organelles and cells at a resolution of a few nanometres. In the last decade ET has allowed major breakthroughs that have provided exciting insights into a wide range of biological processes. In ET the biological sample is imaged with an electron microscope, and a series of images is taken from the sample at different views. Prior to imaging, the sample has to be specially prepared to withstand the conditions within the microscope. Subsequently, those images are processed and combined to yield the three-dimensional reconstruction or tomogram. Afterwards, a number of computational steps are necessary to facilitate the interpretation of the tomogram, such as noise reduction, segmentation and analysis of subvolumes. As the computational demands are huge in some of the stages, high performance computing (HPC) techniques are used to make the problem affordable in reasonable time. This article intends to comprehensively review the methods, technologies and tools involved in the different computational stages behind structural studies by ET, from image acquisition to interpretation of tomograms. The HPC techniques usually employed to cope with the computational demands are also briefly described. © 2012 Elsevier Ltd.},
   author = {Jose Jesus Fernandez},
   doi = {10.1016/J.MICRON.2012.05.003},
   issn = {0968-4328},
   issue = {10},
   journal = {Micron},
   keywords = {Computational methods,Electron tomography,High performance computing,Image processing,Tomographic reconstruction},
   month = {10},
   pages = {1010-1030},
   pmid = {22658288},
   publisher = {Pergamon},
   title = {Computational methods for electron tomography},
   volume = {43},
   year = {2012},
}
@article{Lawrence2006,
   abstract = {Alignment of the individual images of a tilt series is a critical step in obtaining high-quality electron microscope reconstructions. We report on general methods for producing good alignments, and utilizing the alignment data in subsequent reconstruction steps. Our alignment techniques utilize bundle adjustment. Bundle adjustment is the simultaneous calculation of the position of distinguished markers in the object space and the transforms of these markers to their positions in the observed images, along the bundle of particle trajectories along which the object is projected to each EM image. Bundle adjustment techniques are general enough to encompass the computation of linear, projective or nonlinear transforms for backprojection, and can compensate for curvilinear trajectories through the object, sample warping, and optical aberration. We will also report on new reconstruction codes and describe our results using these codes. © 2006 Elsevier Inc. All rights reserved.},
   author = {Albert Lawrence and James C. Bouwer and Guy Perkins and Mark H. Ellisman},
   doi = {10.1016/J.JSB.2005.12.012},
   issn = {1047-8477},
   issue = {2},
   journal = {Journal of structural biology},
   keywords = {Albert Lawrence,Animals,Cell Line,Cells,Computer-Assisted,Cultured,Drosophila / cytology,Drosophila / ultrastructure,Electron / methods*,Electron / statistics & numerical data,Extramural,Fibroblasts / ultrastructure,Fourier Analysis*,Image Processing,Imaging,James C Bouwer,Knockout,MEDLINE,Mark H Ellisman,Mice,Microscopy,Muscle,N.I.H.,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Nodaviridae / ultrastructure,PubMed Abstract,Quantum Dots,Research Support,Skeletal / ultrastructure,Software,Software Design,Three-Dimensional,Tomography / methods,Tomography / statistics & numerical data,Transfection,doi:10.1016/j.jsb.2005.12.012,pmid:16542854},
   month = {5},
   pages = {144-167},
   pmid = {16542854},
   publisher = {J Struct Biol},
   title = {Transform-based backprojection for volume reconstruction of large format electron microscope tilt series},
   volume = {154},
   url = {https://pubmed.ncbi.nlm.nih.gov/16542854/},
   year = {2006},
}
@article{Garcia2021,
   abstract = {G protein-coupled receptors (GPCRs) are the largest single family of cell surface receptors encoded by the human genome and they play pivotal roles in co-ordinating cellular systems throughout the human body, making them ideal drug targets. Structural biology has played a key role in defining how receptors are activated and signal through G proteins and β-arrestins. The application of structure-based drug design (SBDD) is now yielding novel compounds targeting GPCRs. There is thus significant interest from both academia and the pharmaceutical industry in the structural biology of GPCRs as currently only about one quarter of human non-odorant receptors have had their structure determined. Initially, all the structures were determined by X-ray crystallography, but recent advances in electron cryo-microscopy (cryo-EM) now make GPCRs tractable targets for single-particle cryo-EM with comparable resolution to X-ray crystallography. So far this year, 78% of the 99 GPCR structures deposited in the PDB (Jan–Jul 2021) were determined by cryo-EM. Cryo-EM has also opened up new possibilities in GPCR structural biology, such as determining structures of GPCRs embedded in a lipid nanodisc and multiple GPCR conformations from a single preparation. However, X-ray crystallography still has a number of advantages, particularly in the speed of determining many structures of the same receptor bound to different ligands, an essential prerequisite for effective SBDD. We will discuss the relative merits of cryo-EM and X-ray crystallography for the structure determination of GPCRs and the future potential of both techniques.},
   author = {Javier García-Nafría and Christopher G. Tate},
   doi = {10.1042/BST20210431},
   issn = {14708752},
   issue = {5},
   journal = {Biochemical Society Transactions},
   month = {11},
   pages = {2345},
   pmid = {34581758},
   publisher = {Portland Press Ltd},
   title = {Structure determination of GPCRs: cryo-EM compared with X-ray crystallography},
   volume = {49},
   url = {/pmc/articles/PMC8589417/ /pmc/articles/PMC8589417/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8589417/},
   year = {2021},
}
@article{Moawad2020,
   abstract = {Technology has been integrated into every facet of human life, and whether it is completely advantageous remains unknown, but one thing is for sure; we are dependent on technology. Medical advances from the integration of artificial intelligence, machine learning, and augmented realities are widespread and have helped countless patients. Much of the advanced technology utilized by medical providers today has been borrowed and extrapolated from other industries. There remains no great collaboration between providers and engineers, which may be why medicine is only in its infancy of innovation with regards to advanced technologic integration. The purpose of this narrative review is to highlight the different technologies currently being utilized in a variety of medical specialties. Furthermore, we hope that by bringing attention to one shortcoming of the medical community, we may inspire future innovators to seek collaboration outside of the purely medical community for the betterment of all patients seeking care.},
   author = {Gaby N. Moawad and Jad Elkhalil and Jordan S. Klebanoff and Sara Rahman and Nassir Habib and Ibrahim Alkatout},
   doi = {10.3390/JCM9123811},
   issn = {20770383},
   issue = {12},
   journal = {Journal of Clinical Medicine},
   keywords = {Artificial intelligence,Augmented reality,Machine learning,Surgery},
   month = {12},
   pages = {1-7},
   pmid = {33255705},
   publisher = {Multidisciplinary Digital Publishing Institute  (MDPI)},
   title = {Augmented Realities, Artificial Intelligence, and Machine Learning: Clinical Implications and How Technology Is Shaping the Future of Medicine},
   volume = {9},
   url = {/pmc/articles/PMC7761251/ /pmc/articles/PMC7761251/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7761251/},
   year = {2020},
}

@article{Han2021,
   abstract = {Energy-dispersive X-ray spectroscopy (EDX) is often performed simultaneously with high-angle annular dark-field scanning transmission electron microscopy (STEM) for nanoscale physico-chemical analysis. However, high-quality STEM-EDX tomographic imaging is still challenging due to fundamental limitations such as sample degradation with prolonged scan time and the low probability of X-ray generation. To address this, we propose an unsupervised deep learning method for high-quality 3D EDX tomography of core–shell nanocrystals, which can be usually permanently dammaged by prolonged electron beam. The proposed deep learning STEM-EDX tomography method was used to accurately reconstruct Au nanoparticles and InP/ZnSe/ZnS core–shell quantum dots, used in commercial display devices. Furthermore, the shape and thickness uniformity of the reconstructed ZnSe/ZnS shell closely correlates with optical properties of the quantum dots, such as quantum efficiency and chemical stability.},
   author = {Yoseob Han and Jaeduck Jang and Eunju Cha and Junho Lee and Hyungjin Chung and Myoungho Jeong and Tae Gon Kim and Byeong Gyu Chae and Hee Goo Kim and Shinae Jun and Sungwoo Hwang and Eunha Lee and Jong Chul Ye},
   doi = {10.1038/s42256-020-00289-5},
   issn = {25225839},
   issue = {3},
   journal = {Nature Machine Intelligence},
   month = {3},
   pages = {267-274},
   publisher = {Nature Research},
   title = {Deep learning STEM-EDX tomography of nanocrystals},
   volume = {3},
   year = {2021},
}
@article{Kawahara2022,
   abstract = {Atomic-resolution electron microscopy imaging of solid-state material is a powerful method for structural analysis. Scanning transmission electron microscopy (STEM) is one of the actively used techniques to directly observe atoms in materials. However, some materials are easily damaged by the electron beam irradiation, and only noisy images are available when we decrease the electron dose to avoid beam damages. Therefore, a denoising process is necessary for precise structural analysis in low-dose STEM. In this study, we propose total variation (TV) denoising algorithm to remove quantum noise in an STEM image. We defined an entropy of STEM image that corresponds to the image contrast to determine a hyperparameter and we found that there is a hyperparameter that maximizes the entropy. We acquired atomic-resolution STEM image of CaF2 viewed along the [001] direction and executed TV denoising. The atomic columns of Ca and F are clearly visualized by the TV denoising, and atomic positions of Ca and F are determined with the error of ±1 pm and ±4 pm, respectively.},
   author = {Kazuaki Kawahara and Ryo Ishikawa and Shun Sasano and Naoya Shibata and Yuichi Ikuhara},
   doi = {10.1093/JMICRO/DFAC032},
   issn = {2050-5701},
   issue = {5},
   journal = {Microscopy (Oxford, England)},
   keywords = {Algorithms*,Electron,Kazuaki Kawahara,MEDLINE,Microscopy,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Ryo Ishikawa,Scanning Transmission,Yuichi Ikuhara,doi:10.1093/jmicro/dfac032,pmid:35713554},
   month = {10},
   pages = {302-310},
   pmid = {35713554},
   publisher = {Microscopy (Oxf)},
   title = {Atomic-resolution STEM image denoising by total variation regularization},
   volume = {71},
   url = {https://pubmed.ncbi.nlm.nih.gov/35713554/},
   year = {2022},
}

@article{Wastl2013,
   abstract = {Ambient operation poses a challenge to AFM because in contrast to operation in vacuum or liquid environments, the cantilever dynamics change dramatically from oscillating in air to oscillating in a hydration layer when probing the sample. We demonstrate atomic resolution by imaging of the KBr(001) surface in ambient conditions by frequency-modulation atomic force microscopy with a cantilever based on a quartz tuning fork (qPlus sensor) and analyze both long- and short-range contributions to the damping. The thickness of the hydration layer increases with relative humidity, thus varying humidity enables us to study the in uence of the hydration layer thickness on cantilever damping. Starting with measurements of damping versus amplitude, we analyzed the signal and the noise characteristics at the atomic scale. We then determined the optimal amplitude which enabled us to acquire high-quality atomically resolved images.},
   author = {Daniel S. Wastl and Alfred J. Weymouth and Franz J. Giessibl},
   doi = {10.1103/PhysRevB.87.245415},
   issue = {24},
   journal = {Physical Review B - Condensed Matter and Materials Physics},
   keywords = {3420-b,6808-p,6837Ps,numbers: 0779Lh},
   month = {3},
   title = {Optimizing atomic resolution of force microscopy in ambient conditions},
   volume = {87},
   url = {http://arxiv.org/abs/1303.5204 http://dx.doi.org/10.1103/PhysRevB.87.245415},
   year = {2013},
}

@article{Miao2016,
   abstract = {Crystallography has been fundamental to the development of many fields of science over the last century. However, much of our modern science and technology relies on materials with defects and disorders, and their three-dimensional (3D) atomic structures are not accessible to crystallography. One method capable of addressing this major challenge is atomic electron tomography. By combining advanced electron microscopes and detectors with powerful data analysis and tomographic reconstruction algorithms, it is now possible to determine the 3D atomic structure of crystal defects such as grain boundaries, stacking faults, dislocations, and point defects, as well as to precisely localize the 3D coordinates of individual atoms in materials without assuming crystallinity. Here we review the recent advances and the interdisciplinary science enabled by this methodology. We also outline further research needed for atomic electron tomography to address long-standing unresolved problems in the physical sciences.},
   author = {Jianwei Miao and Peter Ercius and Simon J.L. Billinge},
   doi = {10.1126/SCIENCE.AAF2157},
   issn = {0036-8075},
   issue = {6306},
   journal = {Science},
   month = {9},
   pages = {aaf2157-aaf2157},
   publisher = {American Association for the Advancement of Science},
   title = {Atomic electron tomography: 3D structures without crystals},
   volume = {353},
   url = {http://science.sciencemag.org/},
   year = {2016},
}
@article{MacLaren2014,
   abstract = {In recent years, a whole range of fascinating phenomena have been found in oxide materials, or their surfaces or interfaces. These include among many others:• novel electric transport properties at...},
   author = {Ian MacLaren and Quentin M. Ramasse},
   doi = {10.1179/1743280413Y.0000000026},
   issn = {09506608},
   issue = {3},
   journal = {International Materials Reviews},
   keywords = {Aberration-corrected STEM,Atomic resolution,EELS,Functional oxides,HAADF imaging,Spectroscopy},
   month = {4},
   pages = {115-131},
   publisher = {Taylor & Francis},
   title = {Aberration-corrected scanning transmission electron microscopy for atomic-resolution studies of functional oxides},
   volume = {59},
   url = {https://www.tandfonline.com/doi/abs/10.1179/1743280413Y.0000000026},
   year = {2014},
}
@article{Muller2009,
   abstract = {A new generation of electron microscopes is able to explore the microscopic properties of materials and devices as diverse as transistors, turbine blades and interfacial superconductors. All of these systems are made up of dissimilar materials that, where they join at the atomic scale, display very different behaviour from what might be expected of the bulk materials. Advances in electron optics have enabled the imaging and spectroscopy of these buried interface states and other nanostructures with atomic resolution. Here I review the capabilities, prospects and ultimate limits for the measurement of physical and electronic properties of nanoscale structures with these new microscopes.},
   author = {David A. Muller},
   doi = {10.1038/nmat2380},
   issn = {1476-4660},
   issue = {4},
   journal = {Nature Materials 2009 8:4},
   keywords = {Biomaterials,Condensed Matter Physics,Materials Science,Nanotechnology,Optical and Electronic Materials,general},
   pages = {263-270},
   pmid = {19308085},
   publisher = {Nature Publishing Group},
   title = {Structure and bonding at the atomic scale by scanning transmission electron microscopy},
   volume = {8},
   url = {https://www.nature.com/articles/nmat2380},
   year = {2009},
}

@article{Linck2016,
   abstract = {Atomic resolution in transmission electron microscopy of thin and light-atom materials requires a rigorous reduction of the beam energy to reduce knockon damage. However, at the same time, the chromatic aberration deteriorates the resolution of the TEM image dramatically. Within the framework of the SALVE project, we introduce a newly developed Cc/Cs corrector that is capable of correcting both the chromatic and the spherical aberration in the range of accelerating voltages from 20 to 80 kV. The corrector allows correcting axial aberrations up to fifth order as well as the dominating off-axial aberrations. Over the entire voltage range, optimum phase-contrast imaging conditions for weak signals from light atoms can be adjusted for an optical aperture of at least 55 mrad. The information transfer within this aperture is no longer limited by chromatic aberrations. We demonstrate the performance of the microscope using the examples of 30 kV phase-contrast TEM images of graphene and molybdenum disulfide, showing unprecedented contrast and resolution that matches image calculations.},
   author = {Martin Linck and Peter Hartel and Stephan Uhlemann and Frank Kahl and Heiko Müller and Joachim Zach and Max Haider and Marcel Niestadt and Maarten Bischoff and Johannes Biskupek and Zhongbo Lee and Tibor Lehnert and Felix Börrnert and Harald Rose and Ute Kaiser},
   doi = {10.1103/PHYSREVLETT.117.076101/FIGURES/5/MEDIUM},
   issn = {10797114},
   issue = {7},
   journal = {Physical Review Letters},
   month = {8},
   pages = {076101},
   publisher = {American Physical Society},
   title = {Chromatic Aberration Correction for Atomic Resolution TEM Imaging from 20 to 80 kV},
   volume = {117},
   url = {https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.117.076101},
   year = {2016},
}
@article{Krivanek2010,
   abstract = {Direct imaging and chemical identification of all the atoms in a material with unknown three-dimensional structure would constitute a very powerful general analysis tool. Transmission electron microscopy should in principle be able to fulfil this role, as many scientists including Feynman realized early on. It images matter with electrons that scatter strongly from individual atoms and whose wavelengths are about 50 times smaller than an atom. Recently the technique has advanced greatly owing to the introduction of aberration-corrected optics. However, neither electron microscopy nor any other experimental technique has yet been able to resolve and identify all the atoms in a non-periodic material consisting of several atomic species. Here we show that annular dark-field imaging in an aberration-corrected scanning transmission electron microscope optimized for low voltage operation can resolve and identify the chemical type of every atom in monolayer hexagonal boron nitride that contains substitutional defects. Three types of atomic substitutions were found and identified: carbon substituting for boron, carbon substituting for nitrogen, and oxygen substituting for nitrogen. The substitutions caused in-plane distortions in the boron nitride monolayer of about 0.1 magnitude, which were directly resolved, and verified by density functional theory calculations. The results demonstrate that atom-by-atom structural and chemical analysis of all radiation-damage-resistant atoms present in, and on top of, ultra-thin sheets has now become possible. © 2010 Macmillan Publishers Limited. All rights reserved.},
   author = {Ondrej L. Krivanek and Matthew F. Chisholm and Valeria Nicolosi and Timothy J. Pennycook and George J. Corbin and Niklas Dellby and Matthew F. Murfitt and Christopher S. Own and Zoltan S. Szilagyi and Mark P. Oxley and Sokrates T. Pantelides and Stephen J. Pennycook},
   doi = {10.1038/NATURE08879},
   issn = {1476-4687},
   issue = {7288},
   journal = {Nature},
   keywords = {Analytical*,Boron Compounds / chemistry,Chemistry Techniques,Electron / methods*,MEDLINE,Matthew F Chisholm,Microscopy,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-P.H.S.,Ondrej L Krivanek,PubMed Abstract,Research Support,Stephen J Pennycook,U.S. Gov't,doi:10.1038/nature08879,pmid:20336141},
   month = {3},
   pages = {571-574},
   pmid = {20336141},
   publisher = {Nature},
   title = {Atom-by-atom structural and chemical analysis by annular dark-field electron microscopy},
   volume = {464},
   url = {https://pubmed.ncbi.nlm.nih.gov/20336141/},
   year = {2010},
}
@article{Ishizuka1977,
   abstract = {The multislice formulation of Cowley and Moodie for high-energy electron scattering is rederived from the Schrödinger equation, and the validity of the finite slice approach in practical computation is theoretically proved by the stationary-phase approximation. A set of computer programs for the multislice method is developed, where the convolution integral is carried out through the fast Fourier transform. The following conditions are required to obtain a sufficiently accurate result in multislice calculations: (1) the maximum slice thickness should be about kd2, where k is the wavenumber of the incident electrons and d is the distance over which the potential does not change appreciably; (2) there must be a sufficient number of beams in the multislice iteration to prevent the aliasing effect of convolution. The multiple scattering masks the real specimen structure when the specimen thickness exceeds a certain value. This effect of multiple scattering is recognized from the probability distribution of the scattered electrons in addition to the scattering amplitudes obtained through the procedure developed in the present work.},
   author = {K. Ishizuka and N. Uyeda},
   doi = {10.1107/S0567739477001879},
   issn = {0567-7394},
   issue = {5},
   journal = {urn:issn:0567-7394},
   month = {9},
   pages = {740-749},
   publisher = {International Union of Crystallography},
   title = {A new theoretical and practical approach to the multislice method},
   volume = {33},
   url = {//scripts.iucr.org/cgi-bin/paper?a14220},
   year = {1977},
}

@article{Nellist1999,
   abstract = {We use a Bloch wave approach to show that, even for coherent dynamical scattering from a stationary lattice with no absorption, annular dark-field imaging in a scanning transmission electron microscope gives a direct incoherent structure image of the atomic-column positions of a zone-axis- aligned crystal. Although many Bloch waves may be excited by the probe, the detector provides a filtering effect so that the 1s-type bound states are found to dominate the image contrast for typical experimental conditions. We also find that the column intensity is related to the transverses kinetic energy of the 1s states, which gives atomic number, Z, contrast. The additional effects of phonon scattering are discussed, in particular the reasons why phonon scattering is not a prerequisite for transverse incoherence.},
   author = {P. D. Nellist and S. J. Pennycook},
   doi = {10.1016/S0304-3991(99)00017-0},
   issn = {03043991},
   issue = {1-4},
   journal = {Ultramicroscopy},
   keywords = {Electron diffraction and elastic scattering theory,Image simulation,Scanning transmission electron microscopy (STEM)},
   month = {6},
   pages = {111-124},
   publisher = {Elsevier Science Publishers B.V.},
   title = {Incoherent imaging using dynamically scattered coherent electrons},
   volume = {78},
   year = {1999},
}
@article{Barthel2010,
   abstract = {The precise characterisation of the instrumental imaging properties in the form of aberration parameters constitutes an almost universal necessity in quantitative HRTEM, and is underlying most hardware and software techniques established in this field. We focus in this paper on the numerical analysis of individual diffractograms as a first preparatory step for further publications on HRTEM aberration measurement. The extraction of the defocus and the 2-fold astigmatism from a diffractogram is a classical pattern recognition problem, which we believe to have solved in a near-optimum way concerning precision, speed, and robustness. The newly gained measurement precision allows us to resolve fluctuations of the defocus and the 2-fold astigmatism and to assess thereby the optical stability of electron microscopes. Quantitative stability criteria are elaborated, which may serve as helpful guidelines for daily work as well as for microscope acceptance tests. © 2010 Elsevier B.V.},
   author = {J. Barthel and A. Thust},
   doi = {10.1016/J.ULTRAMIC.2010.09.007},
   issn = {1879-2723},
   issue = {1},
   journal = {Ultramicroscopy},
   keywords = {A Thust,J Barthel,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,doi:10.1016/j.ultramic.2010.09.007,pmid:21111264},
   month = {12},
   pages = {27-46},
   pmid = {21111264},
   publisher = {Ultramicroscopy},
   title = {Aberration measurement in HRTEM: implementation and diagnostic use of numerical procedures for the highly precise recognition of diffractogram patterns},
   volume = {111},
   url = {https://pubmed.ncbi.nlm.nih.gov/21111264/},
   year = {2010},
}
@article{Ruskin2013,
   abstract = {A new generation of direct electron detectors for transmission electron microscopy (TEM) promises significant improvement over previous detectors in terms of their modulation transfer function (MTF) and detective quantum efficiency (DQE). However, the performance of these new detectors needs to be carefully monitored in order to optimize imaging conditions and check for degradation over time. We have developed an easy-to-use software tool, FindDQE, to measure MTF and DQE of electron detectors using images of a microscope's built-in beam stop. Using this software, we have determined the DQE curves of four direct electron detectors currently available: the Gatan K2 Summit, the FEI Falcon I and II, and the Direct Electron DE-12, under a variety of total dose and dose rate conditions. We have additionally measured the curves for the Gatan US4000 and TVIPS TemCam-F416 scintillator-based cameras. We compare the results from our new method with published curves. © 2013 Elsevier Inc.},
   author = {Adrian I. Ruskin and Zhiheng Yu and Nikolaus Grigorieff},
   doi = {10.1016/J.JSB.2013.10.016},
   issn = {1095-8657},
   issue = {3},
   journal = {Journal of structural biology},
   keywords = {Adrian I Ruskin,Computer-Assisted / methods*,Electron,Electrons,Extramural,Image Processing,MEDLINE,Microscopy,N.I.H.,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Nikolaus Grigorieff,PMC3876735,PubMed Abstract,Research Support,Software,Transmission / instrumentation,Transmission / methods*,Zhiheng Yu,doi:10.1016/j.jsb.2013.10.016,pmid:24189638},
   month = {12},
   pages = {385-393},
   pmid = {24189638},
   publisher = {J Struct Biol},
   title = {Quantitative characterization of electron detectors for transmission electron microscopy},
   volume = {184},
   url = {https://pubmed.ncbi.nlm.nih.gov/24189638/},
   year = {2013},
}
@article{Faruqi2011,
   abstract = {Electron microscopy (EM) is an important tool for high-resolution structure determination in applications ranging from condensed matter to biology. Electronic detectors are now used in most applications in EM as they offer convenience and immediate feedback that is not possible with film or image plates. The earliest forms of electronic detector used routinely in transmission electron microscopy (TEM) were charge coupled devices (CCDs) and for many applications these remain perfectly adequate. There are however applications, such as the study of radiation-sensitive biological samples, where film is still used and improved detectors would be of great value. The emphasis in this review is therefore on detectors for use in such applications. Two of the most promising candidates for improved detection are: monolithic active pixel sensors (MAPS) and hybrid pixel detectors (of which Medipix2 was chosen for this study). From the studies described in this review, a back-thinned MAPS detector appears well suited to replace film in for the study of radiation-sensitive samples at 300 keV, while Medipix2 is suited to use at lower energies and especially in situations with very low count rates. The performance of a detector depends on the energy of electrons to be recorded, which in turn is dependent on the application it is being used for; results are described for a wide range of electron energies ranging from 40 to 300 keV. The basic properties of detectors are discussed in terms of their modulation transfer function (MTF) and detective quantum efficiency (DQE) as a function of spatial frequency. © 2011 Cambridge University Press.},
   author = {A. R. Faruqi and G. McMullan},
   doi = {10.1017/S0033583511000035},
   issn = {1469-8994},
   issue = {3},
   journal = {Quarterly reviews of biophysics},
   keywords = {A R Faruqi,Cryoelectron Microscopy,Electron / instrumentation*,Electron / methods*,Electrons*,G McMullan,MEDLINE,Microscopy,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Photoelectron Spectroscopy,PubMed Abstract,Review,Time Factors,doi:10.1017/S0033583511000035,pmid:21524337},
   month = {8},
   pages = {357-390},
   pmid = {21524337},
   publisher = {Q Rev Biophys},
   title = {Electronic detectors for electron microscopy},
   volume = {44},
   url = {https://pubmed.ncbi.nlm.nih.gov/21524337/},
   year = {2011},
}
@article{Sang2014,
   abstract = {We report the development of revolving scanning transmission electron microscopy - RevSTEM - a technique that enables characterization and removal of sample drift distortion from atomic resolution images without the need for a priori crystal structure information. To measure and correct the distortion, we acquire an image series while rotating the scan coordinate system between successive frames. Through theory and experiment, we show that the revolving image series captures the information necessary to analyze sample drift rate and direction. At atomic resolution, we quantify the image distortion using the projective standard deviation, a rapid, real-space method to directly measure lattice vector angles. By fitting these angles to a physical model, we show that the refined drift parameters provide the input needed to correct distortion across the series. We demonstrate that RevSTEM simultaneously removes the need for a priori structure information to correct distortion, leads to a dramatically improved signal-to-noise ratio, and enables picometer precision and accuracy regardless of drift rate. © 2013 Elsevier B.V.},
   author = {Xiahan Sang and James M. LeBeau},
   doi = {10.1016/J.ULTRAMIC.2013.12.004},
   issn = {0304-3991},
   journal = {Ultramicroscopy},
   keywords = {Drift correction,Image distortion,Projective standard deviation (PSD),Revolving STEM (RevSTEM),Scanning transmission electron microscopy (STEM),Signal to noise ratio},
   month = {3},
   pages = {28-35},
   publisher = {North-Holland},
   title = {Revolving scanning transmission electron microscopy: Correcting sample drift distortion without prior knowledge},
   volume = {138},
   year = {2014},
}
@article{Ishizuka1980,
   author = {Kazuo Ishizuka},
   issn = {0304-3991},
   issue = {1-3},
   journal = {Ultramicroscopy},
   pages = {55-65},
   title = {Contrast transfer of crystal images in TEM},
   volume = {5},
   url = {https://www.academia.edu/18916287/Contrast_transfer_of_crystal_images_in_TEM},
   year = {1980},
}
@article{Falsini2023,
   abstract = {Halide perovskites are a novel class of semiconductors that have attracted great interest in recent decades due to their peculiar properties of interest for optoelectronics. In fact, their use ranges from the field of sensors and light emitters to ionizing radiation detectors. Since 2015, ionizing radiation detectors exploiting perovskite films as active media have been developed. Recently, it has also been demonstrated that such devices can be suitable for medical and diagnostic applications. This review collects most of the recent and innovative publications regarding solid-state devices for the detection of X-rays, neutrons, and protons based on perovskite thin and thick films in order to show that this type of material can be used to design a new generation of devices and sensors. Thin and thick films of halide perovskites are indeed excellent candidates for low-cost and large-area device applications, where the film morphology allows the implementation on flexible devices, which is a cutting-edge topic in the sensor sector.},
   author = {Naomi Falsini and Alberto Ubaldini and Flavio Cicconi and Antonietta Rizzo and Anna Vinattieri and Mara Bruzzi},
   doi = {10.3390/S23104930},
   issn = {1424-8220},
   issue = {10},
   journal = {Sensors 2023, Vol. 23, Page 4930},
   keywords = {X,films,halide perovskite,ionizing radiation,rays,solid,state detectors},
   month = {5},
   pages = {4930},
   pmid = {37430844},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Halide Perovskites Films for Ionizing Radiation Detection: An Overview of Novel Solid-State Devices},
   volume = {23},
   url = {https://www.mdpi.com/1424-8220/23/10/4930/htm https://www.mdpi.com/1424-8220/23/10/4930},
   year = {2023},
}
@article{Jones2015,
   abstract = {Many microscopic investigations of materials may benefit from the recording of multiple successive images. This can include techniques common to several types of microscopy such as frame averaging to improve signal-to-noise ratios (SNR) or time series to study dynamic processes or more specific applications. In the scanning transmission electron microscope, this might include focal series for optical sectioning or aberration measurement, beam damage studies or camera-length series to study the effects of strain; whilst in the scanning tunnelling microscope, this might include bias-voltage series to probe local electronic structure. Whatever the application, such investigations must begin with the careful alignment of these data stacks, an operation that is not always trivial. In addition, the presence of low-frequency scanning distortions can introduce intra-image shifts to the data. Here, we describe an improved automated method of performing non-rigid registration customised for the challenges unique to scanned microscope data specifically addressing the issues of low-SNR data, images containing a large proportion of crystalline material and/or local features of interest such as dislocations or edges. Careful attention has been paid to artefact testing of the non-rigid registration method used, and the importance of this registration for the quantitative interpretation of feature intensities and positions is evaluated.},
   author = {Lewys Jones and Hao Yang and Timothy J. Pennycook and Matthew S.J. Marshall and Sandra Van Aert and Nigel D. Browning and Martin R. Castell and Peter D. Nellist},
   doi = {10.1186/S40679-015-0008-4/FIGURES/11},
   issn = {21980926},
   issue = {1},
   journal = {Advanced Structural and Chemical Imaging},
   keywords = {Image registration,Non-rigid registration,Principle component analysis,Quantitative ADF,Scanning tunnelling microscopy (STM),Strain mapping},
   month = {12},
   pages = {1-16},
   publisher = {Springer},
   title = {Smart Align—a new tool for robust non-rigid registration of scanning microscope data},
   volume = {1},
   url = {https://ascimaging.springeropen.com/articles/10.1186/s40679-015-0008-4},
   year = {2015},
}
@article{Boulanger2007,
   abstract = {We present a novel space-time patch-based method for image sequence restoration. We propose an adaptive statistical estimation framework based on the local analysis of the bias-variance trade-off. At each pixel, the space-time neighborhood is adapted to improve the performance of the proposed patch-based estimator. The proposed method is unsupervised and requires no motion estimation. Nevertheless, it can also be combined with motion estimation to cope with very large displacements due to camera motion. Experiments show that this method is able to drastically improve the quality of highly corrupted image sequences. Quantitative evaluations on standard artificially noise-corrupted image sequences demonstrate that our method outperforms other recent competitive methods. We also report convincing results on real noisy image sequences. © 2007 IEEE.},
   author = {Jérôme Boulanger and Charles Kervrann and Patrick Bouthemy},
   doi = {10.1109/TPAMI.2007.1064},
   issn = {01628828},
   issue = {6},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Bias-variance trade-off,Denoising,Image sequence restoration,Nonlinear filtering,Nonparametric estimation},
   month = {6},
   pages = {1096-1102},
   pmid = {17431307},
   title = {Space-time adaptation for patch-based image sequence restoration},
   volume = {29},
   url = {https://www.researchgate.net/publication/6397823_Space-Time_Adaptation_for_Patch-Based_Image_Sequence_Restoration},
   year = {2007},
}
@article{cicek2016,
   abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup,the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup,we assume that a representative,sparsely annotated training set exists. Trained on this data set,the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch,i.e.,no pre-trained network is required. We test the performance of the proposed method on a complex,highly variable 3D structure,the Xenopus kidney,and achieve good results for both use cases.},
   author = {Özgün Çiçek and Ahmed Abdulkadir and Soeren S. Lienkamp and Thomas Brox and Olaf Ronneberger},
   doi = {10.1007/978-3-319-46723-8_49/TABLES/3},
   isbn = {9783319467221},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {3D,Biomedical volumetric image segmentation,Convolutional neural networks,Fully-automated,Semi-automated,Sparse annotation,Xenopus kidney},
   pages = {424-432},
   publisher = {Springer Verlag},
   title = {3D U-net: Learning dense volumetric segmentation from sparse annotation},
   volume = {9901 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-46723-8_49},
   year = {2016},
}
@article{Gondara,
   abstract = {Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye.},
   author = {Lovedeep Gondara},
   isbn = {1608.04667v2},
   keywords = {Image denoising,convolu-tional autoencoder,denoising autoencoder},
   title = {Medical image denoising using convolutional denoising autoencoders},
}
@article{McMillan1995,
   abstract = {Image-based rendering is a powerful new approach for generating real-time photorealistic computer graphics. It can provide convinc- ing animations without an explicit geometric representation. We use the “plenoptic function” of Adelson and Bergen to provide a concise problem statement for image-based rendering paradigms, such as morphing and view interpolation. The plenoptic function is a param- eterized function for describing everything that is visible from a given point in space. We present an image-based rendering system based on sampling, reconstructing, and resampling the plenoptic function. In addition, we introduce a novel visible surface algorithm and a geometric invariant for cylindrical projections that is equiva- lent to the epipolar constraint defined for planar projections.},
   author = {Leonard McMillan and Gary Bishop},
   doi = {10.1145/218380.218398},
   keywords = {CR Descriptors: I33 [Computer Graphics]: Picture/Image Gen-eration-display algorithms, viewing algorithms,I37 [Computer Graphics]: Three-Dimensional Graphics and Realism-hidden line/ surface removal,I43 [Image Processing]: Enhancement-regis-tration,I47 [Image Processing]: Feature Measurement-projections,I48 [Image Processing]: Scene Analysis},
   pages = {39-46},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Plenoptic modeling},
   url = {https://dl.acm.org/doi/10.1145/218380.218398},
   year = {1995},
}
@article{Sim2016,
   abstract = {Summary Noise on scanning electron microscope (SEM) images is studied. Gaussian noise is the most common type of noise in SEM image. We developed a new noise reduction filter based on the Wiener filter. We compared the performance of this new filter namely adaptive noise Wiener (ANW) filter, with four common existing filters as well as average filter, median filter, Gaussian smoothing filter and the Wiener filter. Based on the experiments results the proposed new filter has better performance on different noise variance comparing to the other existing noise removal filters in the experiments. SCANNING 38:148-163, 2016.},
   author = {K. S. Sim and V. Teh and M. E. Nia},
   doi = {10.1002/SCA.21250},
   issn = {1932-8745},
   issue = {2},
   journal = {Scanning},
   keywords = {K S Sim,M E Nia,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,V Teh,doi:10.1002/sca.21250,pmid:26235517},
   month = {3},
   pages = {148-163},
   pmid = {26235517},
   publisher = {Scanning},
   title = {Adaptive noise Wiener filter for scanning electron microscope imaging system},
   volume = {38},
   url = {https://pubmed.ncbi.nlm.nih.gov/26235517/},
   year = {2016},
}
@article{Elad2006,
   abstract = {We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods. © 2006 IEEE.},
   author = {Michael Elad and Michal Aharon},
   doi = {10.1109/TIP.2006.881969},
   issn = {10577149},
   issue = {12},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Bayesian reconstruction,Dictionary learning,Discrete cosine transform (DCT),Image denoising,K-SVD,Matching pursuit,Maximum posteriori (MAP) estimation,Redundancy,Sparse representations},
   month = {12},
   pages = {3736-3745},
   pmid = {17153947},
   title = {Image denoising via sparse and redundant representations over learned dictionaries},
   volume = {15},
   year = {2006},
}

@article{Chen2017,
   abstract = {Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems. By embodying recent improvements in nonlinear diffusion models, we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters (i.e., linear filters and influence functions). In contrast to previous nonlinear diffusion models, all the parameters, including the filters and the influence functions, are simultaneously learned from training data through a loss based approach. We call this approach TNRD - Trainable Nonlinear Reaction Diffusion. The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative applications, Gaussian image denoising, single image super resolution and JPEG deblocking. Experiments show that our trained nonlinear diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for the tested applications. Our trained models preserve the structural simplicity of diffusion models and take only a small number of diffusion steps, thus are highly efficient. Moreover, they are also well-suited for parallel computation on GPUs, which makes the inference procedure extremely fast.},
   author = {Yunjin Chen and Thomas Pock},
   doi = {10.1109/TPAMI.2016.2596743},
   issn = {01628828},
   issue = {6},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {JPEG deblocking,Nonlinear reaction diffusion,image denoising,image super resolution,loss specific training},
   month = {6},
   pages = {1256-1272},
   pmid = {27529868},
   publisher = {IEEE Computer Society},
   title = {Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration},
   volume = {39},
   year = {2017},
}
@article{Xin2020,
   abstract = {In this paper, we propose a novel deep convolutional neural network to solve the general multi-modal image restoration (MIR) and multi-modal image fusion (MIF) problems. Different from other methods based on deep learning, our network architecture is designed by drawing inspirations from a new proposed multi-modal convolutional sparse coding (MCSC) model. The key feature of the proposed network is that it can automatically split the common information shared among different modalities, from the unique information that belongs to each single modality, and is therefore denoted with CU-Net, i.e., Common and Unique information splitting network. Specifically, the CU-Net is composed of three modules, i.e., the unique feature extraction module (UFEM), common feature preservation module (CFPM), and image reconstruction module (IRM). The architecture of each module is derived from the corresponding part in the MCSC model, which consists of several learned convolutional sparse coding (LCSC) blocks. Extensive numerical results verify the effectiveness of our method on a variety of MIR and MIF tasks, including RGB guided depth image super-resolution, flash guided non-flash image denoising, multi-focus and multi-exposure image fusion.},
   author = {Xin Deng and Pier Luigi Dragotti},
   title = {Deep Convolutional Neural Network for Multi-modal Image Restoration and Fusion},
}
@misc{Kolesnikov2019,
   abstract = {Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for un-supervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied , but other important aspects, such as the choice of con-volutional neural networks (CNN), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in self-supervised visual representation learning and observe that standard recipes for CNN design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin.},
   author = {Alexander Kolesnikov and Xiaohua Zhai and Lucas Beyer and Google Brain Zürich},
   pages = {1920-1929},
   title = {Revisiting Self-Supervised Visual Representation Learning},
   url = {https://github.com/google/revisiting-self-supervised},
   year = {2019},
}
@article{Chiyu2020,
   abstract = {Shape priors learned from data are commonly used to reconstruct 3D objects from partial or noisy data. Yet no such shape priors are available for indoor scenes, since typical 3D autoencoders cannot handle their scale, complexity, or diversity. In this paper, we introduce Local Implicit Grid Representations, a new 3D shape representation designed for scalability and generality. The motivating idea is that most 3D surfaces share geometric details at some scale -- i.e., at a scale smaller than an entire object and larger than a small patch. We train an autoencoder to learn an embedding of local crops of 3D shapes at that size. Then, we use the decoder as a component in a shape optimization that solves for a set of latent codes on a regular grid of overlapping crops such that an interpolation of the decoded local shapes matches a partial or noisy observation. We demonstrate the value of this proposed approach for 3D surface reconstruction from sparse point observations, showing significantly better results than alternative approaches.},
   author = {Chiyu Max Jiang and Avneesh Sud and Ameesh Makadia and Jingwei Huang and Matthias Niebner and Thomas Funkhouser},
   doi = {10.1109/CVPR42600.2020.00604},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   month = {3},
   pages = {6000-6009},
   publisher = {IEEE Computer Society},
   title = {Local Implicit Grid Representations for 3D Scenes},
   url = {https://arxiv.org/abs/2003.08981v1},
   year = {2020},
}
@article{Xintao2018,
   abstract = {The Super-Resolution Generative Adversarial Network (SR-GAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SR-GAN-network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover , we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN.},
   author = {Xintao Wang and Ke Yu and Shixiang Wu and Jinjin Gu and Yihao Liu and Chao Dong and Yu Qiao and Chen Change Loy},
   title = {ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks},
   url = {https://github.com/xinntao/ESRGAN.},
}
@article{Ledig2017,
   abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image superresolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
   author = {Christian Ledig and Lucas Theis and Ferenc Huszár and Jose Caballero and Andrew Cunningham and Alejandro Acosta and Andrew Aitken and Alykhan Tejani and Johannes Totz and Zehan Wang and Wenzhe Shi},
   doi = {10.1109/CVPR.2017.19},
   isbn = {9781538604571},
   journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   month = {11},
   pages = {105-114},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Photo-realistic single image super-resolution using a generative adversarial network},
   volume = {2017-January},
   year = {2017},
}
@article{Blau2018,
   abstract = {This paper reports on the 2018 PIRM challenge on perceptual super-resolution (SR), held in conjunction with the Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018. In contrast to previous SR challenges, our evaluation methodology jointly quantifies accuracy and perceptual quality, therefore enabling perceptual-driven methods to compete alongside algorithms that target PSNR maximization. Twenty-one participating teams introduced algorithms which well-improved upon the existing state-of-the-art methods in perceptual SR, as confirmed by a human opinion study. We also analyze popular image quality measures and draw conclusions regarding which of them correlates best with human opinion scores. We conclude with an analysis of the current trends in perceptual SR, as reflected from the leading submissions.},
   author = {Yochai Blau and Roey Mechrez and Radu Timofte and Tomer Michaeli and Lihi Zelnik-Manor},
   doi = {10.1007/978-3-030-11021-5_21},
   isbn = {9783030110208},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   month = {9},
   pages = {334-355},
   publisher = {Springer Verlag},
   title = {The 2018 PIRM Challenge on Perceptual Image Super-resolution},
   volume = {11133 LNCS},
   url = {https://arxiv.org/abs/1809.07517v3},
   year = {2018},
}
@article{Mildenhall2021,
   abstract = {Neural Radiance Fields (NeRF) is a technique for high quality novel view synthesis from a collection of posed input images. Like most view synthesis methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images have been processed by a lossy camera pipeline that smooths detail, clips highlights, and distorts the simple noise distribution of raw sensor data. We modify NeRF to instead train directly on linear raw images, preserving the scene's full dynamic range. By rendering raw output images from the resulting NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In addition to changing the camera viewpoint, we can manipulate focus, exposure, and tonemapping after the fact. Although a single raw image appears significantly more noisy than a postprocessed one, we show that NeRF is highly robust to the zero-mean distribution of raw noise. When optimized over many noisy raw inputs (25-200), NeRF produces a scene representation so accurate that its rendered novel views outperform dedicated single and multi-image deep raw denoisers run on the same wide baseline input images. As a result, our method, which we call RawNeRF, can reconstruct scenes from extremely noisy images captured in near-darkness.},
   author = {Ben Mildenhall and Peter Hedman and Ricardo Martin-Brualla and Pratul Srinivasan and Jonathan T. Barron},
   month = {11},
   title = {NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images},
   url = {http://arxiv.org/abs/2111.13679},
   year = {2021},
}

@article{Wang2021,
   abstract = {Considering the problem of novel view synthesis (NVS) from only a set of 2D
images, we simplify the training process of Neural Radiance Field (NeRF) on
forward-facing scenes by removing the requirement of known or pre-computed
camera parameters, including both intrinsics and 6DoF poses. To this end, we
propose NeRF$--$, with three contributions: First, we show that the camera
parameters can be jointly optimised as learnable parameters with NeRF training,
through a photometric reconstruction; Second, to benchmark the camera parameter
estimation and the quality of novel view renderings, we introduce a new dataset
of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset
(BLEFF); Third, we conduct extensive analyses to understand the training
behaviours under various camera motions, and show that in most scenarios, the
joint optimisation pipeline can recover accurate camera parameters and achieve
comparable novel view synthesis quality as those trained with COLMAP
pre-computed camera parameters. Our code and data are available at
https://nerfmm.active.vision.},
   author = {Zirui Wang and Shangzhe Wu and Weidi Xie and Min Chen and Victor Adrian Prisacariu},
   month = {2},
   title = {NeRF--: Neural Radiance Fields Without Known Camera Parameters},
   url = {https://arxiv.org/abs/2102.07064v4},
   year = {2021},
}
 @article{Pearl2022,
   abstract = {Burst denoising is now more relevant than ever, as computational photography
helps overcome sensitivity issues inherent in mobile phones and small cameras.
A major challenge in burst-denoising is in coping with pixel misalignment,
which was so far handled with rather simplistic assumptions of simple motion,
or the ability to align in pre-processing. Such assumptions are not realistic
in the presence of large motion and high levels of noise. We show that Neural
Radiance Fields (NeRFs), originally suggested for physics-based novel-view
rendering, can serve as a powerful framework for burst denoising. NeRFs have an
inherent capability of handling noise as they integrate information from
multiple images, but they are limited in doing so, mainly since they build on
pixel-wise operations which are suitable to ideal imaging conditions. Our
approach, termed NAN, leverages inter-view and spatial information in NeRFs to
better deal with noise. It achieves state-of-the-art results in burst denoising
and is especially successful in coping with large movement and occlusions,
under very high levels of noise. With the rapid advances in accelerating NeRFs,
it could provide a powerful platform for denoising in challenging environments.},
   author = {Naama Pearl and Tali Treibitz and Simon Korman},
   doi = {10.1109/CVPR52688.2022.01234},
   isbn = {9781665469463},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   keywords = {Low-level vision,Physics-based vision and shape-from-X},
   month = {4},
   pages = {12662-12671},
   publisher = {IEEE Computer Society},
   title = {NAN: Noise-Aware NeRFs for Burst-Denoising},
   volume = {2022-June},
   url = {https://arxiv.org/abs/2204.04668v2},
   year = {2022},
}
@INPROCEEDINGS{6168375,
  author={Kushwaha, Himmat S. and Tanwar, Sanju and Rathore, K.S. and Srivastava, Sumit},
  booktitle={2012 Second International Conference on Advanced Computing & Communication Technologies}, 
  title={De-noising Filters for TEM (Transmission Electron Microscopy) Image of Nanomaterials}, 
  year={2012},
  volume={},
  number={},
  pages={276-281},
  doi={10.1109/ACCT.2012.41}}

@mastersthesis{Rakib2024,
  author       = {Rakibuzzaman Mahmud},
  title        = {Neural Radiance Fields for TEM Tomography Images},
  school       = {University of Kiel},
  year         = {2024},
  address      = {Kiel, Germany},
  note         = {Unpublished master's thesis},
  pages        = {40--44}
}
